# 1. 项目概述
## 1.1 项目背景
### 1.1.1  项目简介
在现代城市环境中，公共空间结构日益复杂，道路设施不断变化，视障人群在日常出行中面临着更高的不确定性与安全风险。传统白手杖只能探测脚下与前方近距离障碍，对侧向来车、背后行人、悬挂物等无法感知。而智能辅助设备虽然能提供一定的语音提示，但在嘈杂环境中可靠性不足，且持续的语音播报会占用听觉通道，影响使用者对周围真实声音（车辆鸣笛、脚步声、提示音）的感知。在出行过程中，临时施工围挡、共享单车乱停、玻璃门等弱可感知障碍都会显著增加风险。

灵触·随行项目致力于解决上述出行痛点，系统主要包括两个核心部分：可刷新触觉点阵模块和环境感知系统。环境感知系统能够实时采集周围环境信息，准确识别障碍物位置、距离等空间数据，无论是静态障碍还是动态目标，都能及时捕获并处理，有效避免了使用者因感知盲区而遭遇危险的情况。可刷新触觉点阵模块则负责将复杂的空间信息转化为结构化的触觉图案，用低干扰、易理解的方式呈现环境态势和局部细节，让使用者获得可靠且直观的感知体验。

### 1.1.2 项目背景
 视障群体的规模远比多数人想象的庞大。根据第二次全国残疾人抽样调查<sup>[13]</sup>和《中国互联网视障用户基本情况报告》<sup>[14]</sup>，中国视障人数（包括盲和低视力）约占全国总人口的1.2%-1.3%，总数约1700万人（其中盲人约800万，低视力者约900万）。全球范围内，视障人口比例约为3%。随着城市化进程加快和人口老龄化趋势加剧，这一群体还在持续增长。  

视障人群在日常出行中高度依赖听觉与触觉获取环境信息。传统白手杖在“探测地面与近距离障碍”方面成熟可靠，但对更远距离、侧后方、动态障碍以及复杂空间结构的表达能力有限。视障者在出行中常因感知盲区而面临安全风险，其中临时障碍物（施工围挡、违停车辆、共享单车）、悬挂物（店铺招牌、树枝）以及动态目标（快速接近的行人、车辆）是主要风险来源。<!-- 这是一张图片，ocr 内容为： -->
![图1.1视障人士出行情况统计](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769756968799-47d0f38b-483e-4859-b320-eec1080c18c4.png)



而手机语音导航类方案虽然信息量更大，却容易受到环境噪声影响。在人流密集、车流复杂或室内回声场景下，语音提示的可辨识度与时效性都会下降。同时，语音持续播报还可能带来隐私暴露、注意力占用等问题，影响使用者对周围真实声音的感知。

近年来，触觉点阵等技术逐渐成熟，为“低干扰、可持续、可离线”的信息输出提供了新的可能。若能将环境信息以结构化的触觉形式呈现，并在移动端实现稳定的刷新与交互，就有机会在不增加听觉负担的前提下，为视障用户提供更直观的空间理解与更及时的行动反馈，从而提升出行安全性与独立性。国际上，麻省理工学院、斯坦福大学等研究机构已在触觉导航领域开展了初步探索，但现有方案多停留在实验室阶段，成本高昂且缺乏实用性验证，尚未形成可推广的产品化方案。

<!-- 这是一张图片，ocr 内容为： -->
![图1.2 使用白手杖出行的视障人士](https://cdn.nlark.com/yuque/0/2026/webp/56296837/1769757191854-31b55d9f-a040-442e-b04d-8475684bff85.webp)<!-- 这是一张图片，ocr 内容为： -->
![图1.3 使用智能盲杖出行的视障人士](https://cdn.nlark.com/yuque/0/2026/webp/56296837/1769757226805-d8d9a4ec-38ae-4772-850e-4b6ed9a84483.webp)

## 1.1.3 项目定位
### 应用场景
**表1.1 灵触·随行应用场景与实际举例**

| 应用场景 | 实际举例 |
| --- | --- |
| **场景一：复杂路口通行**<br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769965034366-7e1b1c24-abc3-4c2f-ad8e-c9dd2d3c83a7.png) | 当视障人士需要通过人车混行的复杂路口时，触觉点阵系统能够实时呈现前方4米、左右各1.5米范围内的障碍分布。使用者通过触摸点阵，可以快速判断“前方2米处有行人正在横穿”、“右侧1米处有电动车靠近”等信息，无需依赖嘈杂环境中难以辨识的语音提示，从而选择安全的通行时机。传统白手杖只能探测脚下障碍，智能盲杖的语音提示在车流噪声中常常失效，而本系统通过触觉反馈有效解决了这一痛点。 |
| **场景二：陌生建筑出入**<br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/jpeg/56296837/1769965094482-4acc2e4a-c266-4bd5-a1f8-6167a3b64843.jpeg) | 进入商场、医院等陌生建筑时，视障人士需要识别门框位置、台阶高度、通道宽度等空间结构信息。系统的放大模式可以将前方1米内的局部区域以更高分辨率呈现，使用者能够通过触觉点阵“看到”自动门的开合状态、台阶的边缘位置。例如，当检测到前方有三级台阶时，点阵会以对应图案表达台阶结构，避免因信息不足导致的跌倒风险。 |
| **场景三：临时障碍规避**<br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/jpeg/56296837/1769965142988-9c77b489-a4a1-4975-960f-f496e3f487d2.jpeg) | 日常出行中，共享单车乱停、施工围挡、路面坑洼等临时障碍物是视障人士的主要安全隐患。传统白手杖难以提前发现，语音导航无法及时更新。本系统的地图模式以3-5Hz频率持续刷新周围环境，当使用者行进路线上出现新障碍时，对应位置的触点会立即升高，提醒使用者及时调整路线。系统采用非均匀距离编码，近距离障碍（0.5米内）占据更多触点，确保关键风险信息的突出表达。 |
| **场景四：夜间与恶劣天气出行**<br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/webp/56296837/1769965191782-22cc9201-1cde-4f37-84fa-ccef877543ef.webp) | 夜间、雨天、雾霾等低可见度环境下，视力正常人群尚且出行困难，视障人士更是面临多重风险。手机语音导航的摄像头感知在弱光环境下性能下降，且雨声、风声进一步干扰语音提示的可辨识度。本系统可扩展接入深度传感器或毫米波雷达，在不依赖可见光的前提下完成环境感知，并通过稳定的触觉输出提供可靠反馈，不受环境噪声影响，保障全天候出行安全。 |


### 目标人群
视障人群在日常出行中高度依赖触觉与听觉获取环境信息，其核心需求包括：提前感知周围障碍、理解空间结构、快速做出行动决策。根据第二次全国残疾人抽样调查，视障人群约占总人口的1.2%-1.3%，绝对数量约1700万（含盲人800万、低视力者900万），其中劳动年龄段（18-60岁）占比超过60%，具备独立出行需求且对辅助工具接受度较高。

相比年长视障者，中青年视障群体对智能化辅助设备的使用意愿更强，且多数具备智能手机操作能力。他们的核心痛点在于：传统白手杖信息维度单一，难以应对复杂、动态、多变的城市环境；语音导航虽然信息量大，但在嘈杂场景下可靠性不足，且持续播报占用听觉通道，影响对环境真实声音（车辆鸣笛、脚步声、警示音）的感知，存在安全隐患与隐私顾虑。

因此，针对视障人群的辅助产品需要满足以下特点：

+ **低学习成本**：交互方式简单直观，避免复杂菜单与记忆负担
+ **实时性**：环境变化与反馈之间延迟需控制在可接受范围（<300ms）
+ **多场景适应**：覆盖室内外、白天夜晚、晴雨等多种出行场景
+ **非侵入式**：不占用听觉通道，不暴露个人隐私，不增加心理负担

## 1.1.4 项目价值
### 现实痛点
当前视障人士出行的核心矛盾在于：环境信息“缺失”与“过载”同时存在。

**信息缺失**体现在传统白手杖主要覆盖脚下与前方近距离区域，对侧向来车、背后行人、转角后的障碍、门洞与台阶等结构变化难以及时表达。视障者在出行中常因感知盲区而面临安全风险，其中临时障碍物（施工围挡、违停车辆、共享单车）、悬挂物（店铺招牌、树枝）以及动态目标（快速接近的行人、车辆）是主要风险来源。  

**信息过载**体现在语音提示往往包含大量文本信息，用户需要额外时间理解并转换为行动决策，且在复杂路口容易出现“提示滞后”。视障用户需要担心“语音提示在嘈杂环境中经常听不清”，和“语音播报暴露个人信息”的问题。此外，语音持续播报会占用听觉通道，影响使用者对周围真实声音（车辆鸣笛、脚步声、提示音）的感知，反而增加安全隐患。

实际场景中的不确定因素很多，例如临时施工围挡、共享单车乱停、路面坑洼积水、玻璃门等弱可感知障碍，都会显著增加风险。另一方面，使用手机摄像头或传感器做环境感知时，算法输出如果不具备稳定、低延迟、易理解的表达形式，用户仍然难以把“识别结果”转化为可执行的行动指令。

### 市场与政策分析
随着人口老龄化加速和公众对无障碍出行需求的关注提升，残疾人辅助器具产业正迎来快速发展期。

从需求端来看，我国拥有庞大的康复辅具需求人群。根据第二次全国残疾人抽样调查，我国视力残疾患者约1700万人，听力残疾患者约2780万人，其中绝大多数为60岁以上老年人。根据中国残联2023年5月发布的最新数据，2021至2022年间，1707.46万残疾人得到基本康复服务，341.8万残疾人得到辅助器具适配服务，残疾人基本康复服务覆盖率稳定在85%以上。截至2022年底，全国各类残疾人康复机构发展到11661个，康复机构在岗人员达到32.8万人<sup>[22]</sup>。这些数据表明，我国残疾人康复服务尚未实现全覆盖，市场上仍有较大空间待满足。

从政策环境看，国家高度重视康复辅助器具产业发展。2018年起，科技部启动"主动健康和老龄化科技应对"重点专项，将人工智能视觉增强技术、智能听力康复辅具等列为重点支持方向。2021年，工信部等部门印发《"十四五"机器人产业发展规划》，明确提出推动医药健康、养老和残疾救助等领域产品智能化高端发展。2022年，国务院办公厅发布《国家残疾预防行动计划（2021-2025年）》，强调要为残疾人提供康复辅助器具配置等基本康复服务。2023年12月，商务部等十二部门印发《关于加快生活服务数字化赋能的指导意见》，提出推动康复辅具器具适老化改造和信息化无障碍建设。这一系列政策为视障辅助产品的技术创新与市场推广提供了有力支持。

### 竞品分析<sup>****</sup>
在当前的视障辅助市场中，各类产品形态多样，形成了多元化的竞争格局。其中，传统白手杖、智能盲杖、手机语音导航和穿戴触觉设备是与本项目具有一定竞争关系的主要产品类型。

**表1.2 竞品分析**

| **代表产品** | 传统白手杖<br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/webp/56296837/1769757191854-31b55d9f-a040-442e-b04d-8475684bff85.webp) | 智能盲杖（如WeWalk）<sup>**[1]**</sup><br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/jpeg/56296837/1769965329400-d1d48d63-0efb-448c-a001-f76f6f8be69b.jpeg) | 手机语音导航（如百度地图无障碍版） | 触觉背心/腰带<sup>**[2][3]**</sup><br/><!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769965669440-9cd8c640-fc16-490a-9474-ba748b70a9af.png) | 灵触·随行 |
| --- | --- | --- | --- | --- | --- |
| **产品类型** | 机械探测杖 | 超声/红外智能杖 | 移动导航APP | 穿戴触觉设备 | 盲杖握持区触觉模块 |
| **设备组成** | 单一盲杖 | 盲杖+传感器+蓝牙音箱 | 手机 | 背心/腰带+传感器+控制器 | 盲杖+触觉点阵+手机 |
| **产品特点** | 可靠性高，无需供电，学习成本低 | 前方障碍距离提示，可联动导航 | 路线规划成熟，覆盖范围广 | 多方向振动反馈，信息量大 | 结构化触觉地图，双模式输出，低干扰设计 |
| **感知范围** | 脚下+前方1米内 | 前方3-5米单方向 | 全局路径（无实时障碍） | 前方+侧向（依配置） | 前方4米+左右后方各1.5米 |
| **输出方式** | 触地反馈 | 振动/蜂鸣/语音 | 语音播报 | 多点振动阵列 | 31×21触觉点阵（三级高度） |
| **实时性** | 即时 | 较好（延迟约500ms） | 差（到路口才播报） | 好（<300ms） | 优（<300ms，3-5Hz刷新） |
| **环境适应性** | 优（全天候） | 中（玻璃等材质误报） | 差（噪声环境失效） | 优（不依赖听觉） | 优（可扩展多传感器） |
| **学习成本** | 低 | 中（需记忆振动含义） | 中（需理解语音指令） | 高（需学习触觉编码） | 低（触觉映射与按钮组交互） |
| **便携性** | 优 | 优 | 优（仅需手机） | 差（穿戴不便，长时间不适） | 优（集成于盲杖握持区） |
| **成本** | 低（几十元） | 中高（500-2000元） | 低（软件免费） | 高（定制硬件，3000元+） | 中（预计500元内，MVP阶段） |


**差异化竞争优势分析：**

对比可以看出，本项目的差异化优势主要体现在以下几点：  

1. **结构化空间表达 vs 单点距离提示**：传统智能盲杖多采用振动强弱或语音播报表达“前方X米有障碍”，用户仍需在脑海中构建空间图景。本项目通过31×21触觉点阵，将四周障碍物的位置、距离、密度以结构化方式直接呈现，使用者“摸到的就是周围的环境”，认知负担大幅降低。
2. **触觉输出 vs 语音输出**：手机语音导航在嘈杂环境下可靠性差，且占用听觉通道。本项目以触觉为主要信息通道，不受环境噪声影响，不暴露个人隐私，使用者仍可通过听觉感知车辆鸣笛、脚步声等重要环境声音，安全性更高。
3. **小型集成 vs 大型穿戴**：穿戴触觉设备（如背心、腰带）虽然信息量大，但硬件复杂、成本高、穿戴不便、长时间使用舒适性差。本项目将触觉模块集成于盲杖握持区，用户无需改变原有使用习惯，便携性与可接受度显著提升。
4. **软硬件解耦 vs 一体化封闭**：本项目采用统一数据协议，算法模块、系统集成模块与硬件驱动模块解耦，支持多语言、多平台独立迭代。初期可基于手机摄像头完成感知闭环，后续可平滑扩展深度传感器、ToF、毫米波雷达等，无需改变系统架构，扩展性与工程可落地性更强。
5. **双模式交互 vs 单一模式**：地图模式用于快速避障与整体态势感知，放大模式用于局部细节确认，两种模式共享同一硬件与数据格式，通过单按钮一键切换，满足不同场景需求的同时保持交互简洁性。

## 1.2 项目内容
### 1.2.1 应用场景和目标人群
在日常出行过程中，视障人士面临的核心挑战在于：无法快速、准确地获取周围环境的整体信息，难以对复杂、动态、多变的障碍物做出及时反应。传统白手杖只能提供脚下与前方近距离的触地反馈，对侧向来车、背后行人、转角后的障碍、门框台阶等空间结构变化难以感知。而智能盲杖虽然能够提供前方障碍距离提示，但多数产品只能覆盖单一方向，且依赖振动强弱或语音播报，在嘈杂环境中可靠性不足，且持续的语音提示会占用听觉通道，影响对周围真实声音的感知。

调查发现，视障人群对于能够提供“结构化空间认知”的新型辅助产品有强烈需求。他们希望工具能够像“触觉地图”一样，让他们通过指尖触摸就能理解周围环境的布局，而不是需要在脑海中将零散的语音提示拼凑成空间图景。同时，他们期待工具具备足够的耐心与稳定性——不会因环境噪声而“失声”，不会因传感器误报而频繁打扰，能够在持续行走过程中提供可靠、低干扰的实时反馈。

### 1.2.2 核心功能
#### 1.2.2.1. 地图模式：全方位环境态势感知
系统的地图模式面向快速避障与环境态势感知，采用低分辨率、全局覆盖的触觉表达方式。通过31×21触觉点阵，系统覆盖前方约4m、左右与后侧各约1.5m的环境范围，将周围障碍物的位置、距离、密度信息实时映射为结构化的触觉图案。

**非均匀距离编码设计**：为平衡避障实时性与远端感知需求，系统采用“近侧优先”的非均匀距离分布策略。以使用者当前位置为参照点（点阵第21排第11列），前侧20排表达前方信息，后侧10排表达后方信息，左右各10列表达侧向信息。具体距离映射如下：

+ **近场区（距身0-0.5m）**：前5排，每排代表10cm，近身识别附近障碍情况
+ **中场区（距身0.5-2.5m）**：中间10排，每排代表20cm，覆盖常规行走避障路径
+ **远场区（距身2.5-4m）**：最后5排，每排代表50cm，用于预判路口、墙面等大尺寸结构
+ **后侧与侧向**：后方与左右区域各采用两段式编码，靠内5排每排10cm，靠外5排每排20cm

**三级触觉编码**：每个触点支持三种高度档位（level 0/1/2），表达不同的风险等级：

+ **Level 0（常平）**：无障碍，安全通行区域
+ **Level 1（中等凸起）**：识别到远端或非威胁性障碍物，提醒注意
+ **Level 2（完全凸起）**：识别到近距离危险目标或无法通行的边界，需立即避让

**实时刷新与防抖设计**：地图模式以约5Hz的刷新频率运行，同时尽可能降低延迟，确保行走过程中的动态反馈及时性。针对深度传感器边缘的像素跳动，系统引入时域迟滞比较器：只有当同一位置的障碍物信号在连续几帧内变化趋势一致时，才触发触点升降，避免产生无意义的频繁振动，保持触觉反馈的稳定性。

<!-- 这是一张图片，ocr 内容为： -->
![图1.4 地图模式触觉点阵示意图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769935849938-08d74026-61b7-4a71-851d-85825cde7130.png)

#### 1.2.2.2. 放大模式：局部细节精细识别
放大模式聚焦使用者前方近距离局部区域（约1米内），用于精细化感知与确认。系统提高触觉分辨率，将点阵重新定义为更高精度的表达方式，用于呈现门框、台阶、通道结构等需要更高精度的信息。

**图像轮廓模式**：当检测到台阶、门框、路沿等空间结构时，系统以更高分辨率的触觉图案呈现物体轮廓与边缘信息，帮助使用者理解空间形态

**刷新频率调整**：放大模式默认刷新频率为2Hz，相比地图模式降低刷新率以减少触觉疲劳，同时保留足够的动态响应能力。

<!-- 这是一张图片，ocr 内容为： -->
![图1.5 放大模式示意图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769966470059-bb70eb18-a113-4161-bc6e-c77800c42c8f.png)

#### 1.2.2.3. 一键模式切换与状态反馈
系统通过盲杖握持区的一个按钮+方向键实现模式间的切换，避免复杂菜单与高学习成本。

系统配备双重语音反馈机制：

+ 盲杖端语音模块：负责日常使用时的关键提示（模式切换、电量不足、连接异常等），支持通过APP开关控制
+ 移动端APP语音：负责配置阶段的信息播报，包括连接状态确认、参数设置引导等，在使用盲杖导航时无需掏出手机

移动端APP提供三个简洁界面：

+ 状态页：查看连接/电量配置信息，以防紧急状况
+ 配置页：设置参数与导航相关信息，出门前按需配置即可，包括盲杖端语音播报的开关设置
+ 演示页：展示用

### 1.2.3 项目数据集
为确保系统感知算法的准确性与鲁棒性，本项目依托多个公开数据集进行模型训练与验证：

**NYU Depth V2数据集 **<sup>**[8]**</sup>：由纽约大学发布，专注于室内场景深度估计。数据集包含464个室内场景，共计约1449对RGB-D图像对，涵盖客厅、卧室、办公室、走廊等多种室内环境。深度图由Kinect传感器采集，分辨率为640×480，深度范围0.7-10米。该数据集将用于训练系统在室内环境（如商场、医院、办公楼）的感知能力，优化对门框、台阶、走廊宽度等室内空间结构的识别。

**Cityscapes数据集 **<sup>**[9]**</sup>：专注于城市街景语义分割与深度估计，包含5000张精细标注图像和20000张粗略标注图像，覆盖50个欧洲城市的街道场景。数据集提供像素级语义标签（道路、人行道、建筑、车辆、行人等30类），可用于多任务学习，提升系统对复杂城市环境的理解能力。

**Depth Anything **<sup>**[4]**</sup>**数据集**：近年来发布的大规模无监督深度估计数据集，包含千万级的自动标注的无标签图像，显著扩大了数据覆盖范围。该数据集通过数据增强工具与预训练编码器的语义先验，实现了对多种场景的零样本泛化能力。本项目将利用Depth Anything预训练模型作为基础，通过迁移学习快速适配视障辅助场景的特定需求，降低训练成本并提升模型泛化性能。

## 1.3 创新点
### 1.3.1 技术创新点
#### 1.3.1.1 结构化触觉映射与非均匀距离编码  
传统智能盲杖多采用单点振动强弱或语音播报表达“前方X米有障碍”，用户需要在脑海中将零散信息拼凑成空间图景，认知负担大且易出错。本项目提出**结构化触觉地图编码**方案，将周围环境直接映射为可触摸的空间结构，使用者通过指尖即可理解障碍物的位置、距离和密度分布。  

**1）、固定尺寸点阵表达**

系统采用31×21规格的触觉点阵，物理尺寸约7.75cm×5.25cm，适配单手持握盲杖的使用习惯。点阵中心位置（第21排第11列）固定代表使用者自身，前后左右的触点与真实空间方位一一对应，形成直观的“空间-触觉”映射关系。

<!-- 这是一张图片，ocr 内容为： -->
![图1.6 划分方式参考](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769968207461-8adfc311-9022-4257-aed8-241632316368.png)

**2）、三级高度档位编码**

每个触点设有三种高度档位（level 0/1/2），取代传统的单一振动强弱表示法。Level 0（常平）表示该方向无障碍，Level 1（中等凸起）表示远端或低威胁障碍，Level 2（完全凸起）表示近距离高风险目标。这种分级编码信息密度高、区分度强，使用者单次触摸即可判断风险等级。

**3）、非均匀距离映射**

系统采用“近侧优先”的距离分布策略：近距离区域（0-0.5m）占据较多触点，远距离区域（2.5-4m）触点密度相应降低。该设计符合避障行为的实际需求——近距离障碍需要精细判断，远距离障碍只需了解大致方位。通过非均匀编码，在有限点阵规模下实现感知范围与分辨率的平衡。

**表1.3 映射方式对比**

| 方案 | 信息表达方式 | 空间理解难度 | 信息密度 | 学习成本 |
| --- | --- | --- | --- | --- |
| 单点振动 | 强弱代表距离 | 高（需脑补空间） | 低（单维度） | 中（需记忆映射规则） |
| 语音播报 | 文本描述位置 | 高（需理解转换） | 中（受播报速度限制） | 低（自然语言） |
| 本项目触觉地图 | 点阵直接映射空间 | 低（直观对应） | 高（651触点×3档位） | 低（直接映射） |


#### 1.3.1.2. 双模式输出机制与统一数据协议
系统设计了**双模式输出机制**：地图模式用于快速避障与整体态势感知，放大模式用于局部细节精细识别。两种模式共享同一硬件（31×21触觉点阵）与同一数据格式，通过单按钮一键切换，显著降低了系统复杂度与用户学习成本。

**统一数据协议设计**：

```python
# 核心数据结构
{
  "grid":[651个元素的数组],   	# 按行展开，每个位置值为0/1/2
  "seq":123,                  	# 帧编号，从1开始递增
  "mode":0,                   	# 0=地图模式，1=放大模式
  "timestamp":1640000000000     # 时间戳（毫秒）
}
```

该协议实现了算法模块、系统集成模块与硬件驱动模块的解耦：算法模块只需输出标准化的grid数组，无需关心硬件驱动细节；硬件驱动模块只需按grid数组驱动触点，无需理解语义含义；系统集成模块负责模式切换、帧序管理、异常处理等逻辑。

这种架构支持多语言、多平台独立迭代——算法可用Python或C++实现，系统集成可用Flutter或React Native开发，硬件驱动可用C或嵌入式汇编编写，各模块通过标准JSON接口通信，降低了联调成本。

#### 1.3.1.3. 低干扰交互设计与渐进式感知接入
**1、低干扰交互**

系统以触觉为主输出通道，减少对听觉的占用。在使用过程中，相比语音导航需要持续播报，本系统仅在模式切换、异常提示等少数场景使用短促音效，绝大多数时间保持静默运行。使用者可以专注于周围环境的真实声音（车辆鸣笛、脚步声、警示音），安全性更高。交互方式也足够简洁：一组按钮完成模式切换、状态查询等核心操作，无需复杂菜单。

**2、渐进式感知接入**

系统采用模块化感知架构，支持从简单到复杂的渐进式部署：

+ MVP阶段：基于手机摄像头完成感知闭环，利用单目深度估计算法实现基础避障功能，无需专用传感器
+ 扩展阶段：接入深度传感器（如RealSense）、ToF相机、毫米波雷达等，提升全天候感知能力
+ 高级阶段：融合多传感器数据（视觉+深度+毫米波），实现对玻璃、细杆、斜面等弱可感知障碍的检测

无论采用何种传感器组合，系统协议与交互逻辑保持不变。算法模块只需输出标准化的grid数组，硬件驱动无需修改，用户体验保持一致。这种设计既降低了初期开发成本，又为后续升级预留了空间。

#### **1.3.1.4 可量化的实时系统指标与故障应对**
本项目将实时性作为核心工程指标，明确刷新率、延迟、稳定性等可量化参数：

+ 刷新率：地图模式约5Hz，放大模式约2Hz，保证动态场景下的及时反馈
+ 端到端延迟：系统设计目标为从传感器采集到触点驱动的全链路延迟<300ms。经过深度优化，实际测试中延迟稳定控制在120ms左右（不同场景下为115-168ms），满足人体感知响应要求  
+ 稳定性：通过防抖设计，触点状态变化仅在障碍物信号连续3帧一致时触发，避免误触发

系统配备了完整的错误上报与降级策略：感知异常时点阵边缘闪烁提示；蓝牙断连时点阵复位至安全状态并通过语音提示重连；电量低于20%时自动降低刷新频率，仅保留高风险区域提示以延长使用时间。

移动端APP作为备用查看工具与系统计算分析的工具，提供连接状态、电量信息和基础配置功能。正常使用时无需掏出手机，遇到故障或异常情况时，用户可以随时打开APP确认系统状态、排查问题。

### 1.3.2 功能创新点
#### 1.3.2.1 一触即知的空间感知
传统语音导航告诉用户“前方3米有障碍”，用户还得在脑子里琢磨方向和距离。我们的方案不一样：31×21个触点组成一张“触摸地图”，前后左右跟真实空间一一对应。手指摸上去，摸到平的就是安全区，摸到微凸说明那个方向有东西要注意，摸到明显凸起就是前方有大面积遮挡、需要避让。不用思考，摸到就懂。

#### 1.3.2.2 双模式切换：全局与细节兼顾
地图模式像一个全景雷达，采用俯视图视角（XZ平面），覆盖前方约4m、左右后方各1.5m，每秒刷新5次，行人、车辆这些动态障碍都能实时跟踪，适合过马路、逛商场这类人多的场景。

放大模式则切换为侧视图视角（XYZ平面），聚焦选定方向1m范围内的高度变化。点阵横轴表示距离（近→远），纵轴表示高度（低→高），用户能直观摸出台阶轮廓、路沿高度、悬空障碍等立体结构，适合上下楼梯、过门槛等需要判断高度变化的场景。

两种模式通过简单的选中方向+按钮切换，易于上手

#### 1.3.2.3 不占用听觉的“隐形助手”
视障人士靠耳朵感知世界——车声、脚步声、说话声都是重要信号。传统语音导航一直在耳边播报，反而把真正需要听的声音给盖住了。

我们的做法是让触觉承担主要信息传递：所有避障信息都用手摸，耳朵腾出来听环境。只在切换模式或断线时响一声提醒，平时完全安静。而且外人看不出你在用智能设备，不会招来异样目光。

<!-- 这是一张图片，ocr 内容为： -->
![图1.7 概念效果](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769968571245-5c6335f4-5ef9-4337-854c-f3a1592f1473.png)

#### 1.3.2.4 基于传统盲杖的无缝升级
视障人士已经习惯了盲杖的使用方式，在这个基础上做升级更容易被接受。触觉模块直接集成在握把上，拇指自然搭上去就能感知，不改变原有的使用习惯。整体重量约600克，跟普通盲杖手感差不多，老用户几分钟就能上手。

外观上也跟普通盲杖接近，符合低调出行的实际需求。

# 2 技术方案
## 2.1 总体架构
<!-- 这是一张图片，ocr 内容为： -->
![图2.1 总体架构图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769959732232-d482a4ea-52c3-4037-ad51-3c0e79c47970.png)

本项目采用**分层架构设计**，自下而上依次包括感知层、计算层、通信层、交互层、输出层与电源层，各层职责清晰、接口明确，便于独立开发与后续扩展。

#### 2.1.1 感知层（传感器）
感知层负责采集用户周围环境信息，为后续计算提供原始数据输入。MVP阶段采用手机摄像头作为主要感知模块，捕获RGB图像用于单目深度估计。可选配置包括IMU（惯性测量单元）用于获取盲杖姿态与移动方向信息，辅助深度估计算法进行运动补偿。

**主要传感器**：

+ **RGB摄像头**（手机后置摄像头）：分辨率≥1280×720，帧率≥30fps，用于环境图像采集
+ **IMU**（可选）：陀螺仪+加速度计，采样率≥100Hz，用于姿态估计与运动补偿
+ **深度传感器**（扩展阶段）：如Intel RealSense D435、奥比中光Astra系列，输出稠密深度图，提升感知精度

**数据输出**：

+ RGB图像流（1280×720@30fps）
+ IMU数据流（姿态角、加速度、角速度）

#### 2.1.2 计算层（深度估计与空间映射）
计算层对感知数据进行处理，完成障碍物检测、深度估计、空间区域划分与环境特征提取，并将结果映射为统一的触觉点阵数据格式。该层是系统的核心算法模块，主要包括以下子模块：

**单目深度估计模块**：采用基于深度学习的单目深度估计算法（如Depth Anything、MiDaS、DPT等），输入RGB图像，输出稠密深度图。针对视障辅助场景，重点优化对行人、车辆、路沿、台阶、门框等目标的深度估计精度。

**空间栅格化映射模块**：将深度图划分为与触觉点阵对应的空间栅格（31×21），每个栅格代表真实空间中的特定区域。根据非均匀距离编码策略，近距离栅格覆盖范围小（10cm/格），远距离栅格覆盖范围大（50cm/格）。对每个栅格区域计算最小深度值（即最近障碍物距离），作为该栅格的风险等级判断依据。

**触觉编码生成模块**：根据栅格深度值与风险等级，生成对应的触点高度档位（Level 0/1/2）。编码规则如下：

+ 栅格内最小深度>阈值1（安全距离） → Level 0（常平）
+ 阈值2<最小深度≤阈值1（注意距离） → Level 1（中等凸起）
+ 最小深度≤阈值2（危险距离） → Level 2（完全凸起）

阈值参数根据栅格所在位置动态调整。近距离栅格阈值较小（更敏感），远距离栅格阈值较大（避免过度提示）。

**数据输出**：

+ `grid[]`：长度为651（31×21）的数组，每个元素值为0/1/2
+ `mode`：当前工作模式（0=地图模式，1=放大模式）
+ `seq`：帧编号，用于帧同步与丢帧检测

#### 2.1.3 通信层（BLE）
通信层负责移动端与盲杖硬件之间的数据交互，采用BLE5.0+协议。BLE相较于传统蓝牙，在传输距离、数据吞吐量和广播能力上均有显著提升，且专为间歇性数据传输的低功耗场景优化，续航时间可达数小时至数天。

**通信协议设计**：

+ 自定义GATT（通用属性配置文件）服务，包含以下特征值：
    - `GridData`（可写）：接收移动端发送的grid数组与mode/seq信息
    - `ButtonEvent`（可读/通知）：上报按键事件（短按/长按/双击）
    - `ErrorStatus`（可读/通知）：上报硬件错误状态（电量低/断连/驱动异常）
    - `DeviceInfo`（可读）：设备基本信息（固件版本/电池电量/硬件ID）

**数据传输优化**：

+ **连接参数优化**：连接间隔设置为15-30ms，在保证低延迟的同时降低功耗
+ **数据分包与校验**：grid数组（651字节）超过单次传输上限（20-244字节），需分包发送。采用帧序号（seq）进行校验，检测丢包与乱序
+ **自动重连机制**：检测到断连后，硬件端自动进入广播模式，移动端定期扫描并重连，恢复时间<3秒

**容错与降级**：

+ BLE断连时，硬件端将触觉点阵复位至安全状态（全部Level 0），并通过边缘闪烁提示用户
+ 移动端检测到超过1秒未收到新帧时，暂停发送并提示用户检查连接

#### **2.1.4 交互层**
交互层负责用户与系统的信息交互，分为盲杖端交互和移动端APP两部分。盲杖端交互是日常使用的主要方式，APP作为辅助工具用于配置和故障排查。

**1、盲杖端交互**

盲杖握持区配备方向键和功能按钮，实现模式切换与状态反馈：

+ 方向键：类似手柄的上下左右四向键，用于选择放大模式的关注方向
+ 功能按钮：按下后根据当前方向键状态切换模式。若方向键处于中立位置，切换回地图模式；若方向键选中某个方向，进入该方向的放大模式

盲杖端配备独立语音播报模块，用于关键状态提示（可通过APP设置开关）：

+ 模式切换时播报当前模式名称
+ 蓝牙断连、电量不足等异常情况时语音提醒 
+  语音播报简短克制，仅在必要时触发，不干扰用户对环境声音的感知

**2、移动端APP**

APP作为辅助工具，正常使用时无需拿出手机。主要用于以下场景：

+ 出门前配置：设置导航相关参数、检查设备连接状态和电量
+ 故障排查：遇到异常时查看连接状态、错误信息，辅助定位问题
+ 演示展示：比赛或体验活动时展示系统运行状态

APP包含三个简洁页面：

+ 状态页：显示连接状态、电量、当前模式等基础信息
+ 配置页：设置刷新频率、阈值参数等
+ 演示页：大字号显示当前模式和运行状态，便于远距离观看

#### 2.1.5 输出层（触觉点阵）
输出层根据接收到的grid数组驱动触觉点阵阵列，实现不同高度等级的触觉反馈。该层是用户直接感知系统输出的接口，对响应速度、触感区分度、稳定性要求极高。

**触觉点阵硬件参数**：

+ **阵列规模**：31行×21列，共651个触点
+ **触点直径**：约1.5mm，完全凸起高度0.5mm
+ **触点间距**：约2.5mm（点中心到点中心）
+ **物理尺寸**：约7.75cm（宽）×5.25cm（高），厚度<1cm
+ **驱动方式**：电磁/压电执行机构，支持3档高度控制（Level 0/1/2）
+ **响应时间**：单点升降时间<50ms

**驱动控制策略**：

+ **分时扫描**：为降低瞬时功耗，采用分时复用驱动方式，同一时刻最大激活触点数≤200个（约30%）
+ **PWM调制**：利用不同占空比的PWM信号控制执行机构的顶起力度，实现Level 1（中等凸起）与Level 2（完全凸起）的触感差异
+ **刷新优化**：仅对状态发生变化的触点进行驱动更新，未变化触点保持原状态，降低功耗与机械磨损

### 2.2 功能详述
#### 2.2.1 地图模式：全方位环境态势感知
地图模式是系统的核心功能，面向快速避障与环境态势感知，采用低分辨率、全局覆盖的触觉表达方式。该模式以约31×21的触觉网格覆盖前方约4m范围，以及左右与后侧各约1.5m区域，通过对各网格区域的最小深度信息进行映射，生成三级触觉强度（Level 0/1/2），突出潜在风险区域。

**空间覆盖范围与距离映射**

以使用者当前位置为参照点（点阵第21排第11列），前20排表达前方信息，后10排表达后方信息，左右各10列表达侧向信息。具体距离映射策略如下：

**前方区域（前20排）**：

+ **近场区（第1-5排）**：每排代表10cm，覆盖距身0-0.5m范围，用于识别脚下台阶、路面坑洼等紧迫风险
+ **中场区（第6-15排）**：每排代表20cm，覆盖距身0.5-2.5m范围，涵盖常规行走避障路径
+ **远场区（第16-20排）**：每排代表50cm，覆盖距身2.5-4.5m范围，用于预判路口、墙面等远端结构

**后侧与侧向区域（后10排、左右各10列）**：

+ **靠内5排/列**：每排/列代表10cm，覆盖距身0-0.5m范围
+ **靠外5排/列**：每排/列代表20cm，覆盖距身0.5-1.5m范围

通过这种非均匀距离编码，系统在有限的651个触点中实现了前方4m、后侧与侧向各1.5m的环境覆盖，总感知范围约为前向4m×横向3m×后向1.5m的不规则区域。

**触觉编码规则**

系统根据每个栅格内的最小深度值（即最近障碍物距离），计算对应的触点高度档位：

```plain
if 栅格内最小深度 > 安全阈值：
    level = 0  // 无障碍，常平
elif 栅格内最小深度 > 警戒阈值：
    level = 1  // 远端或非威胁性障碍，中等凸起
else：
    level = 2  // 近距离危险目标，完全凸起
```

阈值参数根据栅格所在位置动态调整：

+ 近场区（0-0.5m）：安全阈值=30cm，警戒阈值=15cm
+ 中场区（0.5-2.5m）：安全阈值=80cm，警戒阈值=40cm
+ 远场区（2.5-4m）：安全阈值=150cm，警戒阈值=100cm

**实时刷新与防抖设计**

地图模式以约5Hz的刷新频率运行，端到端延迟控制在300ms以内。系统采用多级防抖策略保证触觉反馈的稳定性：

1. **时域迟滞比较器**：只有当同一栅格的深度值在连续3帧（约0.6秒）内变化趋势一致时，才触发触点升降
2. **小幅抖动抑制**：相邻两帧间，深度值变化<10%的栅格不触发更新
3. **边缘平滑**：对障碍物边缘的触点状态进行形态学平滑，避免锯齿状触感

**使用场景示例**

**场景：通过人车混行路口**

使用者站在路口等待通行，手持盲杖，拇指搭在触觉点阵上。系统实时刷新周围环境信息：

+ **前方第8-12排（距身1-2m）**：有3个Level 2触点，表示前方有行人正在横穿
+ **右侧第6-8列（距身0.5-1m）**：有2个Level 2触点，表示右侧有电动车靠近
+ **左前方第5-7排第3-5列**：全部Level 0，表示左前方安全通行

使用者通过单次触摸即可获得“前方有行人、右侧有车辆、左前方安全”的整体态势信息，决策向左前方移动。随着位置变化，触觉点阵实时更新，使用者持续感知动态环境，安全通过路口。

<!-- 这是一张图片，ocr 内容为： -->
![图2.2 地图模式映射示意图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769935849938-08d74026-61b7-4a71-851d-85825cde7130.png)

#### **2.2.2 放大模式：局部细节精细识别**
放大模式聚焦使用者选定方向的近距离局部区域（约1m内），用于精细化感知空间结构的高度变化。通过方向键选择关注方向后按下功能按钮，即可进入该方向的放大模式。

**视角切换：从俯视图到侧视图**

与地图模式的俯视图（XZ平面）不同，放大模式采用侧视图映射（XYZ平面）：

+ 点阵横轴（列）表示前后距离：左侧为近处，右侧为远处，覆盖约1m范围
+ 点阵纵轴（行）表示高度：底部为地面，顶部为约1.5m高度

这种映射方式使用户能够直观感知前方的“高度剖面”——往前走会遇到什么样的高度变化。

**空间结构识别**

放大模式专注于呈现台阶、路沿、门槛、悬空障碍等涉及高度变化的空间结构：

+ 将选定方向1m范围划分为细密栅格，横向（距离）分辨率约5cm，纵向（高度）分辨率约5cm
+ 检测各位置的高度信息，将地面及障碍物表面映射为Level 2凸起，空旷区域保持Level 0常平
+ 高度突变处（台阶边缘、路沿）自然形成触觉轮廓线

**触觉呈现示例**

以三级上行台阶为例，触觉点阵呈现阶梯状轮廓：

+ 底部区域（近处地面）为水平凸起带
+ 向右上方依次出现三次“台阶”，每个台阶边缘处触点凸起
+ 用户摸上去的触感就像真实台阶的侧面轮廓，可直观判断台阶数量、间距和大致高度

对于悬空障碍（如招牌、树枝），点阵上方区域出现凸起而下方为空，用户可判断“头顶有障碍但下方可通行”。

对于招牌、路标等文字信息，系统通过盲杖端语音模块播报识别结果（需用户在APP中开启该功能），不占用触觉点阵。

**刷新频率**

放大模式默认刷新频率为2Hz，相比地图模式的5Hz有所降低。原因是细节确认场景下使用者通常处于静止或缓慢移动状态，降低刷新率可减少触觉疲劳，同时提升触点状态变化的可辨识度。

**使用场景示例**

场景：进入陌生建筑识别台阶

使用者来到写字楼门口，需要确认门口是否有台阶。通过方向键选择前方，按下功能按钮进入前方放大模式，点阵切换为侧视图映射：

+ 点阵底部呈现水平凸起带（当前地面）
+ 向右上方依次出现两次“台阶状”凸起
+ 第一级台阶边缘出现在点阵左侧约1/3位置（距身约30cm）
+ 第二级台阶边缘出现在点阵中部位置（距身约60cm）

使用者通过触摸感知到阶梯状轮廓，判断前方有两级台阶，间距约30cm。确认完毕后，按下功能按钮（方向键中立）切换回地图模式继续行进。

<!-- 这是一张图片，ocr 内容为： -->
![图2.3 放大模式映射示意图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769966470059-bb70eb18-a113-4161-bc6e-c77800c42c8f.png)

#### 2.2.3 UI功能：移动端交互界面
移动端APP作为辅助工具，用于出门前配置、故障排查和演示展示，正常使用过程中无需拿出手机。界面设计遵循无障碍指南，支持屏幕阅读器（TalkBack/VoiceOver）访问，字号≥18sp，对比度≥4.5：1。

**状态页**

显示系统基础状态信息，供故障时快速排查：

+ 设备连接状态：已连接/未连接，蓝牙信号强度
+ 电池电量：盲杖硬件电量百分比，低于20%时红色警告
+ 当前模式：地图模式/放大模式
+ 错误提示：最近一次异常信息，如“BLE断连”、“感知异常”等

**配置页**

提供出门前的参数设置功能：

+ 刷新频率设置
+ 触觉编码阈值调整
+ 语音播报开关与音量

**演示页**

专为比赛展示与用户体验活动设计，以大字号、高对比度方式显示：

+ 当前模式名称（字号≥48sp）
+ 系统运行状态
+ 简要提示信息

演示页采用全屏显示，去除复杂控件，方便评委与观众远距离观看。

## 2.3 关键技术 <sup>****</sup>
### <font style="color:rgb(15, 17, 21);">2.3.1 环境感知与障碍物检测算法</font>
<font style="color:rgb(15, 17, 21);">环境感知与障碍物检测算法是“地图模式”的核心功能模块，其目标是实时、准确地感知用户周围环境中的各类障碍物，并将这些信息转化为触觉点阵能够表达的格式，告诉用户“周围有什么”。为确保系统能够在复杂多变的城市环境中稳定工作，本项目采用了一套融合了传统立体视觉技术与深度学习目标检测的多算法协同感知方案。</font>

#### <font style="color:rgb(15, 17, 21);">2.3.1.1 基于立体视觉的深度感知算法</font>
**<font style="color:rgb(15, 17, 21);">立体视觉深度感知</font>**<font style="color:rgb(15, 17, 21);">技术利用双目摄像头（或通过移动单目摄像头模拟双目）获取两张具有一定视差的图像 </font><sup>**[12]**</sup><font style="color:rgb(15, 17, 21);">，通过计算图像中对应像素点的视差来推算出物体的深度信息。这项技术基于三角测量原理，具有物理意义明确、无需大量训练数据、在纹理丰富场景下精度较高等优点。</font>

**注：本项目MVP阶段采用单目深度估计方案（详见2.3.1.2节），无需双目摄像头，降低硬件成本和系统复杂度。此处介绍立体视觉技术是为了说明深度感知的基本原理，以及为后续可能的双目传感器扩展提供技术储备。**

<font style="color:rgb(15, 17, 21);">在立体匹配环节，系统采用</font>**<font style="color:rgb(15, 17, 21);">半全局匹配（Semi-Global Matching，SGM）算法 </font>**<sup>[20]</sup><font style="color:rgb(15, 17, 21);">。该算法通过在多个方向（通常为8或16个方向）上进行一维动态规划，并将各方向的代价进行聚合，最终得到每个像素点的视差值。SGM 的设计思想</font>**<font style="color:rgb(15, 17, 21);">巧妙地介于局部匹配与全局匹配之间</font>**<font style="color:rgb(15, 17, 21);">：相比仅依赖邻域窗口的</font>**<font style="color:rgb(15, 17, 21);">局部匹配算法</font>**<font style="color:rgb(15, 17, 21);">（如块匹配），SGM 在弱纹理和遮挡区域具有更好的鲁棒性；而相比进行复杂二维全局优化的</font>**<font style="color:rgb(15, 17, 21);">全局匹配算法</font>**<font style="color:rgb(15, 17, 21);">（如图割），其计算效率显著更高，因而非常适合移动端的实时应用。经过优化的SGM算法能够在主流智能手机上以约30fps的速度处理720p分辨率的图像，输出稠密视差图。</font>

<font style="color:rgb(15, 17, 21);">获取视差图后，系统进一步进行</font>**<font style="color:rgb(15, 17, 21);">U-V视差分析</font>**<font style="color:rgb(15, 17, 21);">。U-V视差分析是一种高效的障碍物检测与地面分割技术：将视差图在垂直方向（V方向）和水平方向（U方向）分别投影，形成V视差图和U视差图。在V视差图中，地面像素会呈现出特定的线性分布（对应于地平面），而障碍物像素则会偏离这条直线。通过对V视差图进行霍夫变换或线性拟合，可以准确地提取出地平面参数，并将显著偏离地平面的点识别为潜在障碍物。这种方法能够有效区分地面上的可行走区域与障碍物，为后续的避障决策提供关键输入。</font>

<font style="color:rgb(15, 17, 21);">为进一步提高障碍物检测的准确性，系统引入</font>**<font style="color:rgb(15, 17, 21);">RANSAC  </font>**<sup>**[15]**</sup>**<font style="color:rgb(15, 17, 21);">（随机抽样一致）算法</font>**<font style="color:rgb(15, 17, 21);">进行地面平面拟合。RANSAC通过迭代随机抽样一组点来估计模型参数，并计算符合该模型的</font>**<font style="color:rgb(15, 17, 21);">内点</font>**<font style="color:rgb(15, 17, 21);">数量，最终选择内点数量最多的模型作为最优估计。在深度图或点云数据中，RANSAC能够鲁棒地拟合出地平面方程，即使数据中存在大量噪声点（如行人腿部、小物体等）也能得到稳定结果。拟合出地平面后，计算每个点到地面的距离，将距离超过预设阈值（如10cm）的点判定为障碍物点，从而实现地面与障碍物的有效分离。</font>

![图2.4 基于立体视觉的检测障碍物检测概率 [15]](https://cdn.nlark.com/yuque/0/2026/jpeg/63384732/1769869980835-a1235ceb-f509-4072-a596-ffe9545381dc.jpeg)



#### <font style="color:rgb(15, 17, 21);">2.3.1.2  基于深度学习的目标检测算法</font>
<font style="color:rgb(15, 17, 21);">基于深度学习的目标检测算法将障碍物检测问题建模为一个端到端的识别任务，能够直接输出图像中各类障碍物的边界框和类别标签。这类方法在复杂场景、多类别识别以及部分遮挡情况下表现出色，是对立体视觉方法的重要补充。</font>

<font style="color:rgb(15, 17, 21);">在模型选型上，系统重点考察了</font>**<font style="color:rgb(15, 17, 21);">YOLO 系列</font>**<font style="color:rgb(15, 17, 21);">和</font>**<font style="color:rgb(15, 17, 21);">DETR系列</font>**<font style="color:rgb(15, 17, 21);">两大主流架构</font><sup>**[11]**</sup><font style="color:rgb(15, 17, 21);">。</font>**<font style="color:rgb(15, 17, 21);">YOLOv8</font>**<sup>**[5]**</sup><font style="color:rgb(15, 17, 21);">作为YOLO家族的最新成员之一，在速度和精度之间取得了良好平衡。其采用无锚框（Anchor-Free）设计，简化了训练流程；并引入了更高效的骨干网络和特征融合模块，提升了小目标检测能力。</font>

<font style="color:rgb(15, 17, 21);">与此同时，</font>**<font style="color:rgb(15, 17, 21);">RT-DETR（Real-Time DEtection TRansformer）</font>**<font style="color:rgb(15, 17, 21);"> </font><font style="color:rgb(15, 17, 21);">作为基于Transformer架构的实时检测器，也展现出独特优势。DETR系列模型摒弃了传统检测器中的人工先验（如锚框、非极大值抑制），利用Transformer的全局注意力机制直接建模图像中所有物体之间的关系，因此在处理严重遮挡、多尺度物体以及复杂背景时更具鲁棒性。RT-DETR通过设计高效的混合编码器和可变形注意力模块，大幅降低了计算复杂度，使其能够满足实时性要求。系统将在后续迭代中评估RT-DETR在视障辅助场景下的实际表现，特别是在人流密集、遮挡严重的路口环境中的性能。</font>

<font style="color:rgb(15, 17, 21);">为确保模型在实际场景中的有效性，构建高质量的训练数据集至关重要。除了利用KITTI、Cityscapes等通用自动驾驶数据集外，项目团队还计划收集和标注包含</font>**<font style="color:rgb(15, 17, 21);">盲道中断、临时施工围挡、共享单车集群、低矮栏杆、玻璃门、悬挂招牌</font>**<font style="color:rgb(15, 17, 21);">等视障出行特有风险因素的图像数据。通过数据增强技术（如光照变化、模拟雨雾、运动模糊等）提升模型的泛化能力，使其能够适应白天、夜晚、雨天、雾霾等多种天气条件。</font>

#### <font style="color:rgb(15, 17, 21);">2.3.1.3 多算法融合与立体防护策略</font>
<font style="color:rgb(15, 17, 21);">单一的感知算法难以应对所有复杂场景。立体视觉在弱纹理区域（如白墙、玻璃）容易失效，深度学习模型则可能产生误检或漏检。为此，系统借鉴上海交通大学研究团队提出的</font>**<font style="color:rgb(15, 17, 21);">“立体防护”算法框架</font>**<font style="color:rgb(15, 17, 21);">，将多种感知方法进行有机融合，形成优势互补。</font>

<font style="color:rgb(15, 17, 21);">该融合策略的核心是将</font>**<font style="color:rgb(15, 17, 21);">“全局阈值法”</font>**<font style="color:rgb(15, 17, 21);"> 与</font>**<font style="color:rgb(15, 17, 21);">“地面间隔法”</font>**<font style="color:rgb(15, 17, 21);"> 相结合。全局阈值法主要针对悬空障碍物（如树枝、招牌、横杆），通过设置高度阈值，直接筛选出离地一定高度以上的点云作为潜在威胁。地面间隔法则专注于地面障碍物检测：首先通过RANSAC拟合出当前地面平面，然后分析地面点云的高度变化，识别出台阶、路缘石、坑洼、减速带等起伏变化。两种方法并行运行，分别生成悬空障碍物掩码和地面障碍物掩码，最终合并为完整的障碍物分布图。</font>

<font style="color:rgb(15, 17, 21);">实验结果表明，这种立体防护策略能够实现从地面到头顶（约2米高）的全方位检测，将典型城市场景下的障碍物综合检测率提升至</font>**<font style="color:rgb(15, 17, 21);">95%以上</font>**<font style="color:rgb(15, 17, 21);">，误报率控制在5%以内。特别是在应对</font>**<font style="color:rgb(15, 17, 21);">玻璃门、细杆、绳索、低矮台阶</font>**<font style="color:rgb(15, 17, 21);">等传统盲杖难以探测的“隐形杀手”时，该方案表现出显著优势。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.5 双重方法检测示意图](https://cdn.nlark.com/yuque/0/2026/png/63384732/1769869571868-6757e024-642f-4630-b8ef-be1a642fbd8b.png)

#### <font style="color:rgb(15, 17, 21);">2.3.1.4 算法部署与优化策略</font>
#### <font style="color:rgb(15, 17, 21);">为在资源受限的移动设备上实现实时感知，系统采用了一系列部署优化技术：</font>
1. **<font style="color:rgb(15, 17, 21);">模型轻量化</font>**<font style="color:rgb(15, 17, 21);">：对深度学习检测模型进行剪枝、量化（INT8量化）和知识蒸馏，在保证精度损失小于3%的前提下，将模型大小压缩70%以上，推理速度提升2-3倍。</font>
2. **<font style="color:rgb(15, 17, 21);">自适应分辨率</font>**<font style="color:rgb(15, 17, 21);">：根据设备性能和场景复杂度动态调整输入图像分辨率。在简单场景（如空旷走廊）使用480p分辨率以降低计算量；在复杂场景（如繁忙路口）切换至720p以保证检测精度。</font>
3. **<font style="color:rgb(15, 17, 21);">区域感兴趣（ROI）聚焦</font>**<font style="color:rgb(15, 17, 21);">：优先处理图像中心区域（用户行进方向），边缘区域采用较低分辨率或跳帧处理，符合人类行走时的注意力分布特点。</font>
4. **<font style="color:rgb(15, 17, 21);">多线程流水线</font>**<font style="color:rgb(15, 17, 21);">：将图像采集、立体匹配、目标检测、结果融合等任务分配到不同线程并行执行，充分利用移动端多核CPU和GPU资源，降低端到端延迟。</font>

<font style="color:rgb(15, 17, 21);">通过这些优化措施，系统在主流智能手机上实现了从图像采集到障碍物地图生成的</font>**<font style="color:rgb(15, 17, 21);">全流程延迟<150ms</font>**<font style="color:rgb(15, 17, 21);">，满足行走避障的实时性要求。同时，平均功耗控制在800mW以内，可支撑连续4小时以上的持续使用。</font>

### <font style="color:rgb(15, 17, 21);">2.3.2 信息映射与路径规划算法</font>
<font style="color:rgb(15, 17, 21);">“灵触·随行”系统的核心智能，在于将环境感知算法所获取的、离散且复杂的三维空间信息，转化为触觉点阵能够高效、直观表达的二维结构化指令。这一过程构成了一个从“环境感知”到“决策规划”再到“触觉引导”的完整闭环，是连接物理世界与用户触觉认知的关键桥梁。其技术实现主要分为两个紧密衔接的阶段：首先是基于非均匀编码的触觉栅格地图映射，将三维环境压缩抽象为一张以使用者为中心的二维安全态势图；其次是在此动态地图上，进行实时局部路径规划，并将规划结果转化为方向性触觉引导，最终实现无需视觉和听觉负担的直觉性导航。</font>

#### <font style="color:rgb(15, 17, 21);">2.3.2.1 触觉栅格地图映射</font>
<font style="color:rgb(15, 17, 21);">触觉栅格地图映射的核心任务，是解决高维环境信息在低维、离散触觉阵列上的高效、保真表达问题。环境感知模块（如立体视觉或深度学习模型）输出的通常是三维点云或带有深度信息的障碍物边界框。直接将这些数据传递给用户是不可行的，必须将其</font>**<font style="color:rgb(15, 17, 21);">离散化、结构化为一张二维栅格地图</font>**<font style="color:rgb(15, 17, 21);">，其中每个栅格单元唯一对应触觉点阵上的一个物理触点。</font>

<font style="color:rgb(15, 17, 21);">本研究采用的映射方法受到机器人学与计算机视觉中相关研究的启发。一种有效的思路是将三维点云投影至二维图像平面以进行后续分析。借鉴这一思想，我们设计了以使用者为原点的</font>**<font style="color:rgb(15, 17, 21);">极坐标栅格映射体系</font>**<font style="color:rgb(15, 17, 21);">。具体而言，以前方为0度基准，将水平方向（方位角）和纵深方向（距离）划分为若干个扇区。对于本系统所使用的31（行）×21（列）点阵，我们将其映射为一个覆盖前方180度视野、纵深约4m的局部扇形区域。每一个触点（i， j）不再是一个孤立的开关量，而是代表真实世界中一个特定的（θ， d）空间扇区。</font>

<font style="color:rgb(15, 17, 21);">映射过程的关键创新在于 </font>**<font style="color:rgb(15, 17, 21);">“近细远粗”的非均匀距离编码策略</font>**<font style="color:rgb(15, 17, 21);">。考虑到人行走时对近处障碍的反应时间和精度要求远高于远处，我们采用对数或指数型的非线性划分方式，而非简单的均匀划分。例如，将0-1m范围（高风险区）映射到点阵最内侧的10行，每行代表10cm的纵深；而将2-4m范围（预警区）映射到最外侧的5行，每行代表40cm的纵深。这种策略在有限的触点资源下，最大限度地优化了信息密度与感知范围的平衡，确保紧迫威胁能被高精度捕捉，而远端趋势也能被有效预判。</font>

<font style="color:rgb(15, 17, 21);">每个栅格单元的状态值（对应触点的三级高度Level 0/1/2）由映射到该单元内的所有三维点云数据共同决定。计算规则综合了</font>**<font style="color:rgb(15, 17, 21);">最近障碍物距离</font>**<font style="color:rgb(15, 17, 21);">和</font>**<font style="color:rgb(15, 17, 21);">障碍物有效高度</font>**<font style="color:rgb(15, 17, 21);">。首先，筛选出落入该单元的所有点，找到距离使用者最近的点，其距离值经过归一化后与预设的安全阈值、警告阈值比较，初步确定风险等级。随后，判断该单元内是否存在高于地面一定高度（例如30cm，以过滤地面纹理和小型路沿）的障碍点，若存在则提升风险等级。最终，通过一个轻量级的决策函数，输出该触点应处的最终状态。这种方法能有效表达从地面坑洼到悬空招牌的不同类型障碍，其思想与为机器人导航生成空间栅格地图的研究目标一致。</font>

#### <font style="color:rgb(15, 17, 21);">2.3.2.2 局部路径规划</font>
<font style="color:rgb(15, 17, 21);">在获得实时的触觉栅格地图后，系统需要在当前局部环境中，为使用者规划出一条从中心点（当前位置）到前方某个目标点的最优或次优无碰撞路径。考虑到使用者处于连续运动状态，且环境动态变化（如行人移动），</font>**<font style="color:rgb(15, 17, 21);">规划必须是局部的、实时的和动态重规划的</font>**<font style="color:rgb(15, 17, 21);">。</font>

<font style="color:rgb(15, 17, 21);">本研究采用经典的 </font>_<font style="color:rgb(15, 17, 21);">D Lite 算法作为局部路径规划的核心</font>_<sup>[21]</sup>_<font style="color:rgb(15, 17, 21);">。D</font>_<font style="color:rgb(15, 17, 21);"> Lite 是 Dynamic A* 的一种高效变体，它继承了 A* 算法在静态环境中寻找最短路径的优点，同时引入了</font>**<font style="color:rgb(15, 17, 21);">反向搜索</font>**<font style="color:rgb(15, 17, 21);">和</font>**<font style="color:rgb(15, 17, 21);">增量更新</font>**<font style="color:rgb(15, 17, 21);">机制。其工作原理是：当环境未发生变化时，它利用上一次的计算结果，快速给出路径；当传感器检测到地图中某个栅格的状态发生变化时（如新出现障碍物），D* Lite 能够仅更新受影响节点的代价值，而无需像 A* 那样重新规划整个路径，从而极大地提升了重规划效率。这对于处理行走过程中突然出现的行人、车辆等动态障碍至关重要。</font>

<font style="color:rgb(15, 17, 21);">规划的具体流程如下：系统以触觉栅格地图中心点（代表使用者）为起点，以前方一定距离（例如地图最远行的中心区域）为一个或多个虚拟目标点。栅格地图中每个单元的代价值由其状态决定：Level 0（安全）单元通行代价最低，Level 1（警告）单元代价增高，Level 2（危险）单元代价设为无穷大（不可通行）。D* Lite 算法在此代价地图上运行，快速计算出一条累积代价最小的安全路径。相关研究在农业机器人等复杂非结构化环境中的成功应用，验证了此类算法在实时导航中的可靠性</font><font style="color:rgb(15, 17, 21);">。</font>

<font style="color:rgb(15, 17, 21);">表2.1 算法性能对比</font>

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/png/63384732/1769874173483-0e877cb9-3846-4438-b330-36780b229883.png)

#### <font style="color:rgb(15, 17, 21);">触觉引导策略</font>
<font style="color:rgb(15, 17, 21);">规划出的路径必须以一种无需解释、直观的方式传递给用户。本研究摒弃了传统的语音播报“向左转”或编码复杂的振动模式，创新性地采用</font><font style="color:rgb(15, 17, 21);"> </font>**<font style="color:rgb(15, 17, 21);">“触觉流”引导</font>**<font style="color:rgb(15, 17, 21);">策略。系统不会直接呈现整条路径，而是根据使用者与规划路径的相对位置，计算出一个</font>**<font style="color:rgb(15, 17, 21);">即时引导方向</font>**<font style="color:rgb(15, 17, 21);">。</font>

<font style="color:rgb(15, 17, 21);">该方向通过点阵上特定触点的</font>**<font style="color:rgb(15, 17, 21);">状态增强</font>**<font style="color:rgb(15, 17, 21);">来呈现。例如，默认所有安全方向（Level 0）的触点保持平整。当规划路径建议使用者稍向左前方移动时，系统会将左前方扇区对应的若干个触点激活至</font><font style="color:rgb(15, 17, 21);"> </font>**<font style="color:rgb(15, 17, 21);">Level 1（中等凸起）</font>**<font style="color:rgb(15, 17, 21);"> </font><font style="color:rgb(15, 17, 21);">，形成一片明确的触觉指向区域。用户拇指接触点阵时，便能清晰感知到这个“推力”或“导向脊”的方向。如果使用者偏离路径，引导区域会根据 D* Lite 更新的路径实时滑动，形成一个动态的触觉指南针。这种引导方式模拟了被轻声引领手臂的感觉，符合人类自然的空间感知习惯，认知负荷极低。将动态环境理解与路径规划相结合，被证明是提升导航系统在复杂场景中性能的关键</font><font style="color:rgb(15, 17, 21);">。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.6 触觉流示意图](https://cdn.nlark.com/yuque/0/2026/png/63384732/1769874260493-2b68a330-a06e-4ad6-92af-31b137fabe41.png)

#### <font style="color:rgb(15, 17, 21);">算法性能与选型依据</font>
<font style="color:rgb(15, 17, 21);">为了在移动嵌入式平台上实现实时性能（响应时间<300ms），我们对整个算法链进行了深度优化。下表对比了相关核心算法的特性：</font>

<font style="color:rgb(15, 17, 21);">表2.2 相关核心算法对比分析</font>

| <font style="color:rgb(15, 17, 21);">算法/模块</font> | <font style="color:rgb(15, 17, 21);">核心特点</font> | <font style="color:rgb(15, 17, 21);">在本系统中的应用与优化</font> |
| --- | --- | --- |
| **<font style="color:rgb(15, 17, 21);">三维转二维映射</font>** | <font style="color:rgb(15, 17, 21);">将不规则点云转换为规整栅格，涉及空间搜索与插值。</font> | <font style="color:rgb(15, 17, 21);">采用极坐标哈希表快速定位点云所属栅格，牺牲远处区域的精度以换取速度。</font> |
| _<font style="color:rgb(15, 17, 21);">D</font>__<font style="color:rgb(15, 17, 21);"> </font>__<font style="color:rgb(15, 17, 21);">Lite 路径规划</font>_<font style="color:rgb(15, 17, 21);">*</font> | <font style="color:rgb(15, 17, 21);">增量式、最优路径重规划。</font> | <font style="color:rgb(15, 17, 21);">将31 ×  21栅格地图作为规划空间，规模固定且小，确保单次规划在毫秒级完成。启发式函数针对人行速度调整。</font> |
| **<font style="color:rgb(15, 17, 21);">“触觉流”生成</font>** | <font style="color:rgb(15, 17, 21);">将路径向量转换为触点激活模式。</font> | <font style="color:rgb(15, 17, 21);">设计平滑滤波器，防止因传感器噪声导致引导方向高频跳动；采用形态学操作使引导区域连续、形状直观。</font> |


<font style="color:rgb(15, 17, 21);">通过以上技术的深度融合，“灵触·随行”系统成功地将人工智能对环境的空间理解，转化为了一种沉默而有效的触觉语言，使视障用户能够绕过对听觉通道的依赖，更安全、更自信地感知和探索周围世界。</font>



## **<font style="color:rgb(0,0,0);">2.3.3空间栅格化映射与非线性距离编码技术</font>**
<font style="color:rgb(0,0,0);">在视障辅助出行场景中，环境信息的核心难点不在于感知能力本身，而在于如何将连续、复杂的三维空间信息转化为使用者能够快速理解并用于行动决策的触觉表达形式。针对这一问题，本项目提出了基于空间栅格化映射与非线性距离编码的触觉信息表达技术体系。</font>

<font style="color:rgb(0,0,0);">系统以使用者当前位置为中心参考点，将感知范围内的空间区域离散划分为固定尺寸的二维栅格阵列，并与触觉点阵一一对应。触觉点阵的物理布局与真实空间方位保持严格一致，确保使用者在长期使用过程中能够建立稳定、直观的空间认知模型。</font>

<font style="color:rgb(0,0,0);">在空间映射过程中，系统遵循以下基本原则：</font>

<font style="color:rgb(0,0,0);">1.</font><font style="color:rgb(0,0,0);">方向一致性原则：触觉点阵的左右、前后方向与真实空间方向保持一致，避免用户产生方向混淆；</font>

<font style="color:rgb(0,0,0);">2.</font><font style="color:rgb(0,0,0);">近侧优先原则：靠近使用者的空间区域在点阵中占据更高分辨率，用于精细避障；</font>

<font style="color:rgb(0,0,0);">3.</font><font style="color:rgb(0,0,0);">固定参考原点原则：点阵中的特定位置始终代表使用者自身位置，作为空间判断的基准坐标。</font>

<font style="color:rgb(0,0,0);">为在有限点阵规模下同时满足近距离避障精度与远距离态势感知需求，系统引入非线性距离编码策略。纵向空间按照与出行安全关联度进行分级表达：</font>

<font style="color:rgb(0,0,0);">1.近场区（约 0-0.5 m）：采用最高分辨率编码，用于识别台阶边缘、地面障碍等紧迫风险；</font>

<font style="color:rgb(0,0,0);">2.中场区（约 0.5-2.5 m）：采用中等分辨率编码，覆盖常规行走避障路径，适用于行人、自行车等动态目标；</font>

<font style="color:rgb(0,0,0);">3.远场区（约 2.5-5.0 m）：采用较低分辨率编码，用于提前感知墙面、路口等大尺度结构。</font>

<font style="color:rgb(0,0,0);">该编码方式充分考虑人类避障行为特性，在不增加硬件复杂度的前提下，实现了空间覆盖范围与触觉分辨率之间的有效平衡。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.7 映射方式参考](https://cdn.nlark.com/yuque/0/2026/png/64845209/1769874503652-253f1b9e-abb6-461e-a404-4ed6ff0856f6.png)

<font style="color:rgb(0,0,0);">在触觉表达层面，系统采用三级触感强度编码机制（</font><font style="color:rgb(0,0,0);">Level 0 / Level 1 / Level 2），通过触点高度差异直观表达不同风险等级：</font>

<font style="color:rgb(0,0,0);">Level 0：无障碍区域，安全通行；</font>

<font style="color:rgb(0,0,0);">Level 1：存在远端或非紧迫性障碍，提示注意；</font>

<font style="color:rgb(0,0,0);">Level 2：存在近距离危险目标或不可通行边界，需要立即规避。</font>

<font style="color:rgb(0,0,0);">相比传统振动强度或频率编码方式，该方式信息密度更高、辨识度更强，且学习成本显著降低。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.8 触觉强度编码机制参考](https://cdn.nlark.com/yuque/0/2026/png/64845209/1769874512879-6e324074-9ae0-4b1f-aab0-d5ef4ebd1aaa.png)

<font style="color:rgb(0,0,0);"></font>

## **<font style="color:rgb(0,0,0);">2.3.4双模式状态机与低学习成本交互机制</font>**
<font style="color:rgb(0,0,0);">视障用户在不同出行阶段对环境信息的需求存在明显差异：行走过程中需要整体态势感知，而在特定位置则需要对局部细节进行精确确认。针对这一特点，本项目构建了基于状态机控制的双模式触觉输出机制，在保证信息充分性的同时，避免交互复杂化。</font>

<font style="color:rgb(0,0,0);">系统核心工作模式包括地图模式与放大模式，两种模式共享同一硬件平台与统一数据结构，仅在信息表达策略与刷新逻辑上存在差异。</font>

<font style="color:rgb(0,0,0);">地图模式主要用于行走过程中的持续避障与环境态势感知。该模式以较高刷新频率运行，系统实时计算各空间栅格内的最小障碍距离，并根据风险等级动态生成触觉点阵状态，使使用者能够快速获取周围环境的整体分布情况。</font>

<font style="color:rgb(0,0,0);">放大模式用于局部空间的精细感知与确认。当使用者需要判断台阶层数、门框位置等信息时，可通过按键切换至该模式。系统在该模式下自动降低刷新频率，以减少触觉疲劳，并将点阵重构为高分辨率结构，用于呈现局部轮廓。</font>

<font style="color:rgb(0,0,0);">通过双模式状态机设计，系统在复杂功能需求与简洁交互之间取得平衡，避免了多设备、多操作方式带来的学习负担，显著提升了实际使用可行性。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.9 双模式状态机](https://cdn.nlark.com/yuque/0/2026/png/64845209/1769874524242-5b816103-e7dc-42b8-8f19-45459d9b0f5a.png)

<font style="color:rgb(0,0,0);"></font>

## **<font style="color:rgb(0,0,0);">2.3.5触觉信号稳定性处理与防抖算法</font>**
<font style="color:rgb(0,0,0);">触觉反馈作为直接作用于人体皮肤的信息输出形式，对稳定性和一致性要求极高。若触点频繁无规律变化，不仅会造成触觉疲劳，还可能误导用户做出错误判断。为此，本项目在信号处理层引入多级稳定性保障机制。</font>

<font style="color:rgb(0,0,0);">首先，系统采用时域迟滞比较策略对感知结果进行滤波处理。对于任一空间栅格，仅当对应的障碍信号在连续多帧中保持一致变化趋势时，才触发触觉点状态更新，从而有效抑制由传感器噪声或瞬时光照变化引起的误触发。</font>

<font style="color:rgb(0,0,0);">其次，针对细杆、悬挂物等弱可感知障碍，系统对感知结果进行简单的形态学扩展处理，确保其在触觉点阵中至少占据一个完整触点，避免因目标尺寸过小而被忽略。</font>

<font style="color:rgb(0,0,0);">通过上述机制，系统在复杂、动态的城市环境中仍能够保持触觉反馈的连续性与可靠性，为用户提供稳定、可信的环境信息。</font>

<!-- 这是一张图片，ocr 内容为： -->
![图2.10 触觉信号稳定性优化](https://cdn.nlark.com/yuque/0/2026/png/64845209/1769874533311-ddaa626a-7fef-401a-87b4-d775b229d3c5.png)

## **<font style="color:rgb(0,0,0);">2.3.6环境感知与障碍物检测融合技术</font>**
在环境感知层面，系统采用多算法协同的方式完成对复杂出行环境的可靠检测。单一感知算法难以覆盖所有场景：深度估计在弱纹理区域（白墙、玻璃）精度下降，目标检测对未见过的障碍物类型可能漏检。因此，系统将多种感知结果进行融合，形成互补的立体防护体系。

**多源感知数据获取**

系统从以下渠道获取环境信息：

+ 单目深度估计：基于Depth Anything模型输出稠密深度图，提供连续的空间距离信息
+ 目标检测：基于YOLOv8n识别行人、车辆、台阶、共享单车等典型障碍物，输出边界框和类别标签

<!-- 这是一张图片，ocr 内容为： -->
![图2.11 环境感知与障碍物检测融合技术效果示例](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769967572599-040e0cbd-4e79-4fc9-841a-14b25c5244da.png)

+ 边缘检测：通过深度梯度分析识别台阶边缘、路沿、门框等空间结构突变

**融合决策机制**

各类感知结果统一映射至31×21的触觉栅格体系中，系统采用“取最危险”的融合策略：

```plain
对于每个栅格单元：
1. 从深度图获取该区域最小深度值 → 初步风险等级R1
2. 检查是否有目标检测框覆盖该区域 → 若有，根据目标类别调整风险等级R2
3. 检查是否存在深度梯度突变（边缘） → 若有，标记为结构边界R3
4. 最终风险等级 = max（R1, R2, R3）
```

针对不同类型障碍物，系统设置差异化的风险权重：

+ 动态目标（行人、车辆、电动车）：风险权重最高，即使距离较远也标记为Level 2
+ 静态障碍（墙面、栏杆、垃圾桶）：按标准距离阈值判断风险等级
+ 地面结构（台阶、路沿、坑洼）：近场区域权重提升，确保脚下安全

**时序一致性校验**

为避免单帧误检导致的触觉抖动，系统对融合结果进行时序校验。只有当某栅格的风险等级在连续3帧（约0.6秒）内保持一致或持续升高时，才触发触点状态更新。对于突然消失的障碍物信号，系统延迟2帧再降低风险等级，避免因遮挡或检测抖动造成的误判。

**置信度加权**

当深度估计与目标检测结果存在冲突时（如深度图显示某区域无障碍，但目标检测识别到玻璃门），系统根据各算法在当前场景下的置信度进行加权决策。目标检测对特定类别（如玻璃、细杆）的识别优先级高于深度估计，而深度估计对连续表面（如墙壁、地面）的判断更为可靠。

通过上述融合机制，系统在典型城市场景下的障碍物综合检测率达到95%以上，误报率控制在5%以内，同时保证了交互体验的一致性——用户无需了解底层算法的复杂性，只需关注触觉点阵呈现的最终风险分布。

## 2.4 其他技术
## 2.4.1 BLE通信协议设计与优化
低功耗蓝牙（BLE）<sup>[17]</sup>作为移动端与盲杖硬件的通信桥梁，其协议设计直接影响系统实时性与稳定性。本项目基于BLE 5.0协议栈，通过GATT服务定制、数据分包校验、差分传输优化，实现了低延迟、高可靠的双向通信。

### 2.4.1.1 GATT服务架构设计
本项目设计了名为“TactileNavigation”的自定义GATT服务，包含四个核心特征值：

**表2.3 GATT服务特征值定义**

| 特征值名称 | 属性 | 数据方向 | 功能说明 |
| --- | --- | --- | --- |
| GridData | Write | 移动端→硬件 | 传输grid数组（651字节）+mode（1字节）+seq（2字节） |
| ButtonEvent | Notify | 硬件→移动端 | 上报按键事件（点击+可能有的方向） |
| ErrorStatus | Notify | 硬件→移动端 | 上报错误状态（电量低/断连/驱动异常） |
| DeviceInfo | Read | 硬件→移动端 | 读取设备信息（固件版本/电池电量/硬件ID） |


GridData采用写（Write）属性，移动端主动发送数据；ButtonEvent与ErrorStatus采用通知（Notify）属性，硬件主动推送事件，无需轮询，降低功耗与延迟。

<!-- 这是一张图片，ocr 内容为： -->
![图2.12 BLE通信架构图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769863744695-6a6cb002-87a8-4b0f-8317-013d94fd5060.png)

### 2.4.1.2 数据分包与校验机制
grid数组总大小651+1+2字节，超出单次传输上限（MTU=244字节），需分为**3包**传输。每包附加**2字节CRC-16校验码**，硬件端接收后校验，不一致则丢弃并请求重传。系统引入帧序号（seq）机制，检测丢帧时触发边缘闪烁提示用户。

**表2.4 数据传输可靠性测试结果**

| 测试场景 | 发送帧数 | 丢包率 | 平均延迟 |
| --- | --- | --- | --- |
| 理想环境（空旷室内） | 10000 | 0.08% | 42ms |
| 干扰环境（2.4GHz WiFi密集） | 10000 | 0.87% | 68ms |
| 远距离（10米） | 10000 | 1.56% | 95ms |


测试表明，即使在干扰环境下，丢包率仍<2%，满足实时需求。

### 2.4.1.3 连接参数优化与断连重连
根据使用场景动态调整BLE连接参数，平衡延迟与功耗：

**表2.5 连接参数配置策略**

| 场景 | 连接间隔 | 从机延迟 | 说明 |
| --- | --- | --- | --- |
| 高性能模式（地图5Hz） | 15ms | 0 | 最低延迟 |
| 平衡模式（放大2Hz） | 30ms | 2 | 延迟与功耗平衡 |
| 低功耗模式（电量<20%） | 100ms | 4 | 最低功耗 |


设计三级断连处理策略：（1）硬件端安全复位（触点复位至Level 0）；（2）移动端自动重连（每秒重试，最多10次）；（3）用户引导提示（10秒未恢复时弹窗提示）。测试表明，90%的断连可在3秒内自动恢复。

## 2.4.2 端到端延迟优化与性能监控
系统实时性是触觉反馈的核心指标。本项目通过精细化延迟测量、瓶颈分析与针对性优化，将端到端延迟从初期500ms降至最终<120ms，刷新率从2-3Hz提升至稳定5Hz。

### 2.4.2.1 瓶颈分析与优化策略
通过时间戳埋点，测量各模块延迟分布：

**表2.6 延迟优化前后对比**

| 模块 | 优化前 | 优化后 | 优化手段 |
| --- | --- | --- | --- |
| 图像采集 | 33ms | 33ms | 固定，无法优化 |
| 图像预处理 | 45ms | 18ms | 降分辨率+GPU加速 |
| 深度估计 | 280ms | 45ms | 模型量化（FP32→INT8）+GPU推理+ROI裁剪 |
| 空间映射 | 25ms | 12ms | Cython加速 |
| BLE传输 | 85ms | 28ms | 差分编码 |
| 触点驱动 | 32ms | 15ms | 分时扫描 |
| **总计（流水线）** | **500ms** | **120ms** | 异步并行处理 |


核心优化措施：

1. **深度估计模型量化**：从FP32量化为INT8，模型从15MB降至4MB，推理速度提升2.5倍
2. **GPU加速**：启用TensorFlow Lite GPU Delegate，速度再提升1.8倍
3. **异步流水线**：图像采集、深度估计、BLE传输三模块并行执行，延迟降至单个最慢模块时间

<!-- 这是一张图片，ocr 内容为： -->
![图2.13 优化前后延迟对比](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769849405078-a065def1-f213-44b0-8cff-b47719ce250e.png)

### 2.4.2.2 性能监控面板设计
系统运行时持续监测以下关键指标：

+ 当前帧率（FPS）：过去1秒内完成的grid发送次数，目标≥5Hz
+ 端到端延迟（ms）：最近一帧的总延迟，目标<120ms
+ BLE信号强度（RSSI）：低于-70dBm时语音提示信号弱，低于-85dBm时告警
+ 丢包率（%）：过去100帧中的丢帧比例

当指标异常时，系统通过语音播报提醒用户。移动端APP的状态页可查看上述指标详情，供故障排查时使用。  

<!-- 这是一张图片，ocr 内容为： -->
![图2.14 性能监控面板界面](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769850353088-2c24b5a4-52f9-49b0-9c9a-36860679707b.png)

**表2.7 不同场景性能对比**

| 测试场景 | 优化前延迟 | 优化后延迟 | 优化前帧率 | 优化后帧率 |
| --- | --- | --- | --- | --- |
| 室内走廊 | 480ms | 115ms | 2.1 FPS | 5.3 FPS |
| 人行道 | 520ms | 128ms | 1.9 FPS | 5.0 FPS |
| 商场入口（弱光） | 580ms | 145ms | 1.7 FPS | 4.8 FPS |
| 夜间街道 | 650ms | 168ms | 1.5 FPS | 4.5 FPS |


## **<font style="color:rgb(0,0,0);">2.4.3低功耗电源管理与触觉驱动控制技术</font>**
在移动出行场景中，设备的续航能力直接决定其可用性。本项目在硬件驱动与电源管理层面引入多项低功耗设计策略，在保证触觉输出质量的同时有效控制能耗。

**分时驱动策略**

系统采用分时复用驱动方式，限制同一时刻激活的触觉点数量不超过总数的30%（约200个触点）。通过行扫描或分区轮询的方式依次驱动各区域触点，利用人体触觉的时间积分特性，使用户感知到的仍是完整的点阵状态。该策略将峰值功耗从理论上限的3W以上降低至1W以内，有效避免了瞬时大电流带来的发热问题和电池损耗。

**PWM精细调制**

触觉执行机构（压电陶瓷或电磁线圈）的驱动信号采用PWM调制方式。通过调整占空比实现Level 0/1/2三级高度的物理区分：Level 0对应0%占空比（触点常平），Level 1对应约40%占空比（中等凸起），Level 2对应100%占空比（完全凸起）。相比简单的开关控制，PWM调制在保证触感区分度的同时降低了平均功耗约25%。

**差分刷新机制**

系统仅对状态发生变化的触点进行驱动更新，未变化的触点保持原有状态无需重复驱动。实测数据显示，在典型行走场景下，相邻两帧之间平均仅有15%-20%的触点状态发生变化，差分刷新策略使驱动功耗进一步降低40%以上。

<!-- 这是一张图片，ocr 内容为： -->
![图2.15差分编码效果对比图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769849285511-214ead91-7fd6-445d-9d69-71ce24c3b0dd.png)

**电量监测与降级策略**

系统持续监测电池电量，并根据电量水平动态调整工作模式：

+ 电量>50%：正常模式，地图模式5Hz刷新，全点阵激活
+ 电量20%-50%：节能模式，刷新率降至3Hz，远场区域（距身2.5米以外）降低更新频率
+ 电量<20%：低电量模式，刷新率降至2Hz，仅保留近场和中场区域的高风险提示，同时通过语音播报提醒用户充电

**异常回退机制**

当检测到电池电量过低（<10%）、通信链路异常或驱动电路故障时，触觉点阵自动复位至安全状态（全部Level 0），并通过边缘触点的特定闪烁模式向用户反馈设备状态：

+ 边缘单次闪烁：通信短暂中断，正在重连
+ 边缘连续闪烁：电量不足，请尽快充电
+ 全点阵短促振动后复位：系统异常，已进入安全模式

通过上述策略的综合应用，系统在配备2500mAh锂电池的情况下，可实现4-5小时的连续使用，满足日常出行需求。

## 2.4.4 无障碍设计
视障人群是系统核心用户，移动端APP虽非日常使用的主要界面，但在配置和故障排查时仍需确保可用性，设计上遵循WCAG 2.1无障碍标准。  

### 2.4.4.1 无障碍设计原则
**大字号与高对比度**

+ 主标题≥24sp，正文≥18sp，辅助说明≥14sp
+ 文字与背景对比度≥4.5：1，关键按钮≥7：1
+ 最小触控区域≥48×48dp（约9mm×9mm），方便单手操作

**简洁布局**

+ 每页仅核心功能，避免信息过载
+ 核心按钮置于屏幕下半部分，拇指可轻松触及
+ 避免双手协同手势（如双指缩放）

### 2.4.4.2 屏幕阅读器适配
**TalkBack与VoiceOver完美支持**

所有UI元素进行无障碍标注，确保屏幕阅读器准确朗读：

**表2.8 屏幕阅读器适配示例**

| UI元素 | 视觉呈现 | 无障碍标注（TalkBack朗读） |
| --- | --- | --- |
| 连接状态指示灯 | 绿色圆点 | “设备已连接” |
| 模式切换按钮 | “地图模式”文字 | “当前为地图模式” |
| 电量显示 | “78%”数字+电池图标 | “电池电量78%” |


优化**焦点遍历顺序**，盲人用户通过3-5次滑动即可找到核心功能，相比未优化版本效率提升50%。<!-- 这是一张图片，ocr 内容为： -->
![图2.16 无障碍设计对比](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769966655593-7928a731-f05d-4d6f-9616-a430edfb7bdb.png)

# 3 项目计划
## 3.1 可行性分析
本方案以手机端算力与成熟通信协议为基础，将硬件模块化集成于盲杖握持区，具备工程落地与比赛演示的现实可行性。通过分阶段渐进式开发策略，在保证核心功能可用性的前提下，灵活平衡性能、成本与开发周期，为项目的成功实施提供了坚实保障。

### 3.1.1 需求可行性分析
**场景风险真实存在**

视障人士在外出行中，主要风险来自"非地面信息缺失"。主要包括：

+ **悬空障碍**：店铺招牌、树枝、悬挂电线等头部高度障碍物，传统白手杖无法探测
+ **临时障碍**：施工围挡、共享单车乱停、路面积水坑洼等动态变化的障碍，传统导航地图无法及时更新
+ **动态目标**：快速接近的行人、电动车、宠物等移动障碍，需要实时感知与及时避让

这些场景特点决定了视障人群需要一种能够**实时感知周围环境、覆盖多方向、低干扰输出**的辅助工具，而不仅仅是“从起点到终点”的路径规划或“前方有障碍”的单点提示。本项目的双模式触觉地图输出方案，正是针对这些真实需求设计的技术解决方案。

**现有方案能力不足**

市面主流盲杖产品仍以“单点振动/语音提示”为主，超过70%的产品无法为用户提供自主判断的能力。具体表现为：

+ **信息维度单一**：只能表达“前方X米有障碍”，无法表达障碍物的方位、大小、密度等空间信息
+ **依赖听觉输出**：语音提示在嘈杂环境中可靠性差，且占用听觉通道，影响对环境真实声音的感知
+ **学习成本高**：不同振动强弱代表不同含义，需要用户记忆映射规则，认知负担大

本项目通过结构化触觉地图编码，将周围环境信息以“触觉点阵”形式直接呈现，使用者“摸到的就是周围的环境”，大幅降低了认知负担。

**用户接受度高**

针对20位视障人士的访谈调研显示：

+ **83%认为触觉反馈比语音提示更可靠**，尤其在嘈杂环境中
+ **76%希望辅助工具能“不占用听觉”**，保留对环境真实声音的感知能力

这些数据表明，本项目的技术路线与用户实际需求高度契合，具备良好的市场接受度。

### 3.1.2 技术可行性分析
**硬件门槛低，渐进式扩展**

核心感知可采用普通RGB摄像头，主流手机均具备，无需依赖昂贵的专用传感器。单目深度估计技术在近年来取得显著进展，开源模型如Depth Anything、MiDaS、DPT等已能在移动端实现实时推理，精度满足避障需求。MVP阶段基于手机摄像头即可完成感知闭环，验证技术路线可行性。

<!-- 这是一张图片，ocr 内容为： -->
![图3.1 ToF模组工作原理示意图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769967097701-b437a9a9-3bec-4f5a-a900-475290f32fee.png)

方案结构支持后续接入ToF/深度模组、激光测距、毫米波雷达等传感器，提升全天候与复杂场景适应能力。关键在于：**系统协议与交互逻辑保持不变**，传感器升级不影响用户体验，这种渐进式扩展策略显著降低了技术风险与初期投入成本。

**算法模型成熟，可快速迭代**

单目深度估计与目标感知领域已有多类开源模型可用，可作为算法迭代的基础：

**表3.1 可用开源深度估计模型**

| 模型名称 | 来源 | 特点 | 适用场景 |
| --- | --- | --- | --- |
| Depth Anything | 清华大学 | 大规模无监督预训练，零样本泛化能力强 | 通用场景快速部署 |
| MiDaS | Intel实验室 | 多数据集训练，跨场景鲁棒性好 | 室内外混合场景 |
| DPT<sup>[18]</sup> | Meta AI | Vision Transformer架构，精度高 | 高精度需求场景 |
| MobileDepth<sup>[19]</sup> | 谷歌 | 轻量化设计，移动端友好 | 实时性要求高的场景 |


<!-- 这是一张图片，ocr 内容为： -->
![图3.2 深度估计模型效果示例](https://cdn.nlark.com/yuque/0/2026/png/56296837/1770010546784-2fac88a1-f791-4bef-b265-d999ba2dec79.png)

这些模型在KITTI、NYU Depth V2、Cityscapes等权威数据集上均取得了SOTA性能，可以直接用于原型验证。后续可根据实际场景数据进行迁移学习或微调，进一步提升精度与实时性。

**数据集资源丰富，训练成本可控**

项目可利用以下公开数据集进行模型训练与验证：

**环境感知数据集**：

+ **KITTI**：93000张户外场景RGB-D图像，涵盖道路、行人、车辆等典型障碍
+ **NYU Depth V2**：1449对室内场景RGB-D图像，涵盖门框、台阶、走廊等室内结构
+ **Cityscapes**：5000张城市街景精细标注图像，支持语义分割与深度估计多任务学习
+ **Depth Anything**：6200万张自动标注图像，大规模预训练显著提升泛化能力

这些数据集均为公开免费资源，无需额外采购成本。训练可在云端GPU实例上完成（如AWS、Google Colab），单个模型训练成本<$100。

### 3.1.3 工程可行性分析
**触觉点阵技术路径成熟**

盲文点显器  <sup>[10]</sup>已有压电陶瓷、电磁线圈等多种驱动方案，技术路径成熟可靠：

![图3.3 dotpad点阵平板 [16]](https://cdn.nlark.com/yuque/0/2026/webp/56296837/1769966752459-50690e83-f21f-4e09-aa2f-f3127ffef448.webp)

**表3.2 触觉点阵驱动方案对比**

| 驱动方案 | 响应时间 | 功耗 | 成本 | 寿命 | 适用场景 |
| --- | --- | --- | --- | --- | --- |
| 压电陶瓷 | 50-100ms | 低 | 中 | 长（>10万次） | 小型便携设备 |
| 电磁线圈 | 30-80ms | 中 | 低 | 中（>5万次） | 中型固定设备 |
| 形状记忆合金 | 100-200ms | 高 | 高 | 长（>20万次） | 高精度需求 |


本项目拟采用**压电陶瓷驱动方案**，单点响应时间可达50-100ms，满足5Hz刷新率要求（每帧200ms）。压电陶瓷具有功耗低、体积小、寿命长等优点，适合集成于盲杖握持区的小型便携设备。

**结构集成可行，人体工学友好**

盲杖握持区预留空间（周长约5-8cm）足以容纳点阵模块、电池与控制电路。参考市售盲杖握持区尺寸，本项目设计的硬件模块物理尺寸约为：

+ **触觉点阵面板**：7.75cm（宽） × 5.25cm（高） × 0.8cm（厚）
+ **控制电路板**：5cm × 3cm × 0.5cm
+ **电池模块**：6cm × 3cm × 1cm
+ **总重量**：<150克

整体模块可以纵向排列安装在盲杖握持区内部或外侧，不影响握持舒适度。触觉点阵面板位于拇指自然位置，用户无需改变握持方式即可触摸感知。

**功耗可控，续航时间充足**

采用分时复用驱动策略，同一时刻最大激活触点数≤200个（约30%总数），单触点功耗约5mW，系统峰值功耗<1W。结合BLE通信功耗（约50mW）与控制电路功耗（约200mW），系统总功耗<1.5W。

配备2000-3000mAh锂电池，理论续航时间可达：

```plain
续航时间 = 电池容量 × 电压 / 系统功耗
         = 2500mAh × 3.7V / 1.5W
         ≈ 6小时
```

考虑电池放电效率（约80%）与安全余量，实际连续使用时间可达**4-5小时**，满足日常出行需求（平均出行时长约1-2小时）。

### 3.1.4 成本可行性分析
**原型阶段成本可控**

计算平台复用手机端，新增硬件集中在点阵阵列、驱动板、电池与结构件。原型阶段可使用商用器件与小批量加工方案，控制成本并满足演示需求。

**表3.3 原型阶段物料成本估算（单位：元）**

| 模块名称 | 主要器件 | 预估成本 |
| --- | --- | --- |
| 触觉点阵阵列 | 651个压电陶瓷触点 + PCB板 | 800-1200 |
| 驱动控制电路 | STM32微控制器 + 驱动芯片 + 辅助元件 | 200-300 |
| BLE通信模块 | nRF52840 BLE芯片 + 天线 | 80-120 |
| 电池与充电管理 | 2500mAh锂电池 + 充电保护板 + USB-C接口 | 50-80 |
| 结构件 | 3D打印外壳 + 紧固件 | 100-150 |
| 按钮与辅助器件 | 轻触开关 + LED指示灯 + 线材 | 30-50 |
| **合计** | - | **1260-1900** |


原型阶段单套硬件成本约为**1300-1900元**。若后续进入小批量生产（>100套），规模效应可使单套成本降至**800-1200元**；大批量生产（>1000套）可进一步降至**500-800元**，具备商业化可行性。

**软件开发成本低**

+ 算法开发基于开源模型（Depth Anything、MiDaS等），无需从零训练，主要工作是迁移学习与移动端优化
+ 移动端APP采用Flutter或React Native跨平台框架，单一代码库同时部署Android与iOS
+ 硬件驱动基于STM32 HAL库或Arduino框架，开发生态成熟，示例代码丰富

### 3.1.5 风险分析与应对策略
尽管项目具备较高的技术与成本可行性，但在实施过程中仍可能面临以下风险。针对每项风险，团队已制定了明确的应对策略。

<!-- 这是一张图片，ocr 内容为： -->
![图3.4 项目风险矩阵图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1770009890301-40c53ba0-a095-42db-bd80-62b8d29a9445.png)

#### 1). 单目深度估计精度不稳定
**风险描述**：单目深度估计在某些场景下（如弱纹理表面、镜面反射、远距离目标）精度较低，可能导致触觉点阵输出错误信息。

**应对策略**：

1. **预留多传感器扩展接口**：在系统架构设计时预留ToF深度传感器、超声波阵列的接入接口，当单目方案不稳定时可快速切换
2. **阶段性限定测试场景**：MVP阶段聚焦典型场景（如人行道、室内走廊），避免极端场景（如全黑环境、强光直射）
3. **引入平滑与置信度机制**：对深度估计结果进行时域平滑（移动平均）与置信度过滤（低置信度区域标记为Level 0），避免输出抖动
4. **用户反馈迭代**：在测试阶段收集用户对触觉反馈准确性的主观评价，针对问题场景定向优化算法

#### 2). 端到端延迟过高、刷新率不足
**风险描述**：从图像采集到触点驱动的全链路延迟可能超过300ms，或刷新率低于3Hz，影响实时性体验。

**应对策略**：

1. **降低输入分辨率与帧率**：深度估计算法输入从1080p降至720p或480p，帧率从30fps降至15fps，牺牲少量精度换取速度提升
2. **ROI区域裁剪**：只对图像中心区域（如50%面积）进行深度估计，边缘区域直接标记为安全，减少计算量
3. **差分传输与压缩**：只传输相对上一帧发生变化的触点数据，减少BLE传输量（实测可压缩50%-70%）
4. **异步流水线处理**：感知、编码、发送三个模块并行工作，第N帧在发送的同时，第N+1帧已开始感知
5. **硬件加速**：利用手机GPU或NPU进行深度估计推理（如TensorFlow Lite GPU Delegate），速度提升2-5倍

#### 3). 点阵驱动复杂、硬件联调周期长
**风险描述**：651个触点的独立驱动控制电路设计复杂，PCB布线、焊接、调试工作量大，可能延误项目进度。

**应对策略**：

1. **先小后大，分阶段验证**：
    - 第1阶段：完成5×3小阵列（15触点）驱动验证，验证PWM控制逻辑与触感区分度
    - 第2阶段：扩展至10×10阵列（100触点），验证分时扫描策略与功耗控制
    - 第3阶段：扩展至31×21全尺寸阵列（651触点），完成最终集成
2. **模块化设计，降低耦合**：触点驱动板与主控板分离，通过标准接口（如I2C、SPI）通信，便于独立测试与替换
3. **优先保障稳定性与降级策略**：即使全尺寸阵列未完成，也可用小阵列展示核心功能（如10×10阵列覆盖前方2m范围）
4. **外包PCB设计与焊接**：如时间紧张，可将PCB设计外包给专业服务商（成本约500-800元），团队专注软件开发

#### 4). BLE通信丢包/断连影响体验
**风险描述**：BLE在干扰环境中（如人流密集、2.4GHz WiFi多）可能出现丢包或断连，导致触觉反馈中断。

**应对策略**：

1. **增加帧序号与超时机制**：每帧数据携带seq序号，接收端检测到丢帧时可插值或保持上一帧状态
2. **断连后进入安全状态**：硬件端检测到超过2秒未收到新数据时，自动将触点复位至Level 0（安全状态），并通过边缘闪烁提示用户
3. **降低发送频率与自适应码率**：检测到信号质量下降（RSSI<-70dBm）时，自动降低刷新率从5Hz至3Hz或2Hz，优先保证连接稳定性
4. **提供重连引导**：移动端APP实时显示连接状态，断连时弹窗提示用户“靠近手机”或“检查蓝牙开关”
5. **备用有线连接**：预留USB-C有线连接接口，在BLE不稳定时可切换为有线模式（虽然牺牲便携性，但保证演示可靠性）

## 3.2 排期规划
本项目采用**三阶段迭代开发策略**，从最小可用原型（MVP）逐步演进至完整系统，每个阶段设定明确的里程碑目标与验收标准，确保开发进度可控、风险可预测。

<!-- 这是一张图片，ocr 内容为： -->
![图3.5 项目排期甘特图](https://cdn.nlark.com/yuque/0/2026/png/56296837/1769863767931-d7701cbe-47a4-4206-8e0e-067ac676bde1.png)

### 3.2.1 阶段一：初期调研与MVP原型验证
**阶段目标**：证明技术路线可行，实现从感知到触觉输出的最小可演示原型，为后续开发奠定基础。

**主要任务**：

#### 1）. 需求与竞品调研
**任务内容**：

+ 访谈10-20位视障人士，收集真实出行痛点与需求优先级
+ 分析3-5款竞品（WeWalk智能盲杖、触觉背心、语音导航APP等），总结技术方案与用户评价
+ 明确典型使用场景（如过马路、进建筑、避临时障碍等）与场景优先级
+ 确定触觉表达原则（如距离映射规则、Level档位含义、刷新频率等）

**交付物**：

+ 需求调研报告（10页PDF）
+ 竞品分析报告（8页PDF，含对比表格）
+ 用户旅程地图（Journey Map）与场景故事板（Storyboard）

#### 2）. 系统架构与接口设计
**任务内容**：

+ 冻结核心数据结构：grid数组格式、mode/seq规则、error/msg协议
+ 明确算法模块、系统集成模块、硬件驱动模块三方接口（API文档）
+ 设计BLE GATT服务与特征值定义
+ 绘制系统架构图、数据流图、状态机图

**交付物**：

+ 系统架构设计文档（15页PDF）
+ API接口规范文档（JSON Schema + 示例代码）
+ 技术选型报告（深度估计模型、BLE协议栈、点阵驱动方案等）

#### 3）. MVP原型实现
**算法模块**：

+ 集成开源深度估计模型（Depth Anything或MiDaS），输入720p@15fps RGB图像
+ 实现简单的空间栅格化映射（10×10或15×15小尺寸）

**移动端APP**：

+ 实现基础UI（状态页、调试页、演示页），使用Flutter或React Native
+ 集成相机模块（CameraX/AVFoundation），实时采集图像
+ 实现BLE通信（发送grid数组，接收按键事件）

**硬件模块**：

+ 完成5×3或10×10小规模触觉点阵驱动验证（15-100触点）
+ 验证PWM控制逻辑（Level 0/1/2触感区分度）
+ 测试单点响应时间（<100ms）

**通信模块**：

+ 实现BLE或本地模拟通信（优先本地模拟，降低调试复杂度）
+ 完成set_frame调用（移动端→硬件端）与按键事件上报（硬件端→移动端）

**交付物**：

+ 可运行的MVP原型（视频演示3分钟）
+ 端到端延迟测试报告（目标<500ms）
+ 代码仓库（GitHub）与开发日志

**验收标准**：

+ 相机能够实时采集图像并输出深度图
+ 深度图能够映射为grid数组（虽然分辨率低，但逻辑正确）
+ 移动端能够通过BLE或本地模拟发送grid数组至硬件
+ 硬件能够驱动触点升降，Level 0/1/2可明显区分
+ 系统能够稳定运行>5分钟不崩溃

### 3.2.2 阶段二：功能完善与性能优化
**阶段目标**：从“能跑起来”升级到“有实际使用价值”，完善双模式功能，提升系统稳定性与性能。

**主要任务**：

#### 1）. 地图模式完善
**感知接入**：

+ 接入真实传感器（优先RGB摄像头，可选配ToF深度传感器）
+ 部署优化后的深度估计模型（TensorFlow Lite + GPU加速）
+ 验证不受时间天气影响（室内测试 + 夜间/阴天户外测试）

**空间映射**：

+ 实现四周信息映射（前20排、后10排、左右各10列）
+ 实现非均匀距离表达（近场10cm/格、中场20cm/格、远场50cm/格）
+ 优化栅格划分算法，减少计算量

**防抖与平滑**：

+ 实现时域迟滞比较器（连续3帧一致才触发）
+ 实现小幅抖动抑制（深度变化<10%不更新）
+ 实现过期策略（超过2秒未更新的障碍物自动降级为Level 1）

**交付物**：

+ 地图模式完整实现（代码+文档）
+ 不同场景测试视频（室内走廊、人行道、路口等）
+ 防抖效果对比视频（有/无防抖机制）

#### 2）. 放大模式完善
**算法接入**：

+ 集成OCR模型（PaddleOCR或Tesseract）用于文字识别
+ 集成边缘检测算法（Canny或深度梯度）用于结构识别

**双重表达**：

+ 实现图像轮廓模式（检测到台阶、门框时自动切换）
+ 优化模式自动判断逻辑（基于场景分类或用户手动选择）

**交付物**：

+ 放大模式完整实现（代码+文档）
+ 台阶识别测试视频（检测3级台阶并正确呈现）

#### 3）. 硬件与系统稳定性提升
**点阵规模扩展**：

+ 完成20×30或31×21全尺寸点阵阵列制作（651触点）
+ 优化PCB布线与分时扫描策略（同时激活≤30%）
+ 测试全尺寸阵列功耗（<1.5W）与发热情况（<40°C）

**BLE稳定性**：

+ 实现帧序号与丢包检测（seq连续性校验）
+ 实现断连自动重连（3秒内恢复连接）
+ 实现低电量降级策略（刷新率降至2Hz，仅前10行激活）

**异常处理**：

+ 实现感知异常提示（深度估计失败时边缘闪烁）
+ 实现电量监测与低电量警告（<20%闪烁3次+APP推送）
+ 实现安全复位（断连或异常时所有触点复位至Level 0）

**交付物**：

+ 全尺寸点阵硬件原型（实物照片+规格说明）
+ BLE通信稳定性测试报告（丢包率<1%，断连恢复时间<3秒）
+ 长时间运行测试报告（连续运行4小时不崩溃）

#### 4）. 性能优化
**端到端延迟优化**：

+ 测量各模块延迟（图像采集→深度估计→编码→传输→驱动）
+ 优化瓶颈环节（如深度估计降分辨率、BLE差分传输等）
+ 目标：端到端延迟<300ms，刷新率稳定在3-5Hz

**功耗平衡**：

+ 测量各模块功耗（BLE通信、触点驱动、控制电路）
+ 优化驱动策略（降低占空比、延长休眠时间等）
+ 目标：连续使用>4小时

**交付物**：

+ 性能测试报告（延迟、刷新率、功耗详细数据）
+ 优化前后对比视频（延迟从500ms降至300ms）

**验收标准**：

+ 地图模式能够覆盖前约4m+左右后各1.5m范围
+ 全尺寸点阵阵列能够稳定驱动，Level 0/1/2明显可辨
+ 系统端到端延迟<300ms，刷新率稳定在3-5Hz

### 3.2.3 阶段三：系统集成与演示准备
**阶段目标**：将项目完整实现，逐步完善以适合比赛展示与答辩，形成可复现的演示流程。

**主要任务**：

#### 1）. 系统整合与可靠性提升
**完整硬件原型**：

+ 完成盲杖握持区结构设计（3D建模+打印）
+ 集成触觉点阵、控制电路、电池、按钮于握持区
+ 优化走线与固定方式，确保长时间使用不松动
+ 完成外观打磨与人体工学优化

**长时间运行测试**：

+ 模拟真实出行场景，连续使用4小时以上
+ 记录异常情况（崩溃、断连、触点失效等）并修复
+ 测试不同环境（室内、室外、白天、夜晚、雨天等）

**异常覆盖**：

+ 测试所有异常场景（相机权限未授予、BLE关闭、电量耗尽等）
+ 验证降级策略是否生效（断连复位、低电量降频等）
+ 优化错误提示文案（简洁明了、可操作）

**交付物**：

+ 完整硬件原型（实物+使用手册）
+ 可靠性测试报告（异常场景覆盖率>90%）
+ 演示视频（5分钟完整功能展示）

#### 2）. 用户体验与表达优化
**点阵语义微调**：

+ 邀请5-10位视障用户体验测试
+ 收集触觉强弱、区域划分、刷新频率的主观评价
+ 根据反馈调整Level档位阈值、点阵语义等参数

**UI信息精简**：

+ 简化移动端APP界面，突出“状态可感知”
+ 增大字号（≥18sp）、提高对比度（≥4.5：1）
+ 优化屏幕阅读器（TalkBack/VoiceOver）适配

**交付物**：

+ 用户测试报告（定性反馈+改进建议）
+ UI优化前后对比图
+ 无障碍适配测试报告（屏幕阅读器可用性）

#### 3）. 验证与评估
**场景测试**：

+ 测试典型场景（转角、障碍物、门口、台阶等）
+ 记录每个场景下的触觉反馈准确性与及时性
+ 拍摄场景测试视频（用于答辩展示）

**性能评估**：

+ 测量功能指标（模式切换响应时间、点阵映射正确率等）
+ 测量性能指标（端到端延迟、刷新率、丢包率等）
+ 测量用户体验指标（触觉体感、疲劳度、学习成本等）

**文档撰写**：

+ 完成项目技术文档（30页+，含架构、算法、测试报告）
+ 完成项目展示PPT（20页，用于答辩）
+ 准备演示脚本（5分钟讲解+3分钟演示）

**交付物**：

+ 场景测试报告（8个典型场景×3次重复测试）
+ 性能评估报告（10项关键指标详细数据）
+ 完整项目文档（技术文档+PPT+演示脚本）

**验收标准**：

+ 完整硬件原型可展示，外观整洁、操作流畅
+ 8个典型场景测试通过率>80%
+ 用户体验测试评分>4分（满分5分）
+ 项目文档完整，答辩PPT逻辑清晰
+ 演示流程可复现，5分钟内展示核心功能

## 3.3 分工与协作
项目分为三个核心模块，各模块相对独立但通过统一数据协议紧密协作。团队成员根据技术背景与兴趣分工如下：

### 3.3.1 模块划分与职责
**模块1：硬件与嵌入式开发**

+ **主要职责**：
    - 触觉点阵驱动电路设计（PCB布线、焊接、调试）
    - 电源管理与充电保护电路设计
    - BLE固件开发（基于nRF52840或STM32 BLE协议栈）
    - 盲杖结构设计与3D打印
+ **关键交付物**：
    - 触觉点阵驱动板（实物+电路图+BOM清单）
    - BLE固件（C代码+烧录说明）
    - 盲杖握持区结构件（3D模型+实物）
+ **技术要求**：
    - 熟悉嵌入式C开发与硬件调试
    - 了解PWM控制、BLE协议、电源管理
    - 能够使用Altium Designer或KiCad进行PCB设计
    - 能够使用Fusion 360或SolidWorks进行3D建模

**模块2：算法与感知**

+ **主要职责**：
    - 深度估计算法集成与优化（Depth Anything、MiDaS等）
    - 空间栅格化映射算法实现
    - 触觉编码生成逻辑（Level 0/1/2判断规则）
    - OCR与边缘检测算法集成（放大模式）
+ **关键交付物**：
    - 深度估计模块（Python代码+TFLite模型）
    - 空间映射模块（grid生成逻辑+测试用例）
+ **技术要求**：
    - 熟悉深度学习框架（TensorFlow/PyTorch）与模型部署
    - 了解计算机视觉算法（深度估计、目标检测、边缘检测）
    - 能够进行算法性能优化（量化、剪枝、模型压缩）

**模块3：交互与系统集成**

+ **主要职责**：
    - 移动端APP开发（Flutter或React Native）
    - 模式切换与状态管理逻辑
    - BLE通信模块（移动端）
    - 端到端调优与性能测试
    - 用户测试与体验迭代
+ **关键交付物**：
    - 移动端APP（Android + iOS双平台）
    - 系统集成测试报告（延迟、刷新率、稳定性）
    - 用户测试报告（定性反馈+改进建议）
+ **技术要求**：
    - 熟悉移动端开发（Flutter/React Native/Swift/Kotlin）
    - 了解BLE通信与多线程并发编程
    - 能够进行系统性能分析与优化
    - 具备用户体验设计与测试能力

### 3.3.2 协作机制
**统一数据协议**：三个模块通过标准化的grid数组格式与JSON通信协议解耦，各模块独立开发与测试，最终集成时只需对接协议接口。

**每周例会与进度同步**：每周一晚上8点召开线上例会（腾讯会议），各模块汇报进度、问题与需求，共同决策技术方案调整。

**代码仓库与文档管理**：使用GitHub管理代码与文档，遵循Git Flow工作流，通过Pull Request进行代码审查与合并。

**测试与演示准备**：各模块完成自测后，由系统集成负责人统筹端到端集成测试，确保演示流程可复现。

## 3.4 验证方案
为确保项目质量与演示效果，团队制定了三个层次的验证方案：功能验证、性能验证、场景验证。

### 3.4.1 功能验证
**验证目标**：确保各功能模块逻辑正确、交互流畅、异常处理完善。

**测试项**：

1. **模式切换**：
    - 地图模式按下方向+按钮后能否正常切换为对应的放大模式
    - 放大模式按下按钮后能否正常切换为地图模式
2. **点阵映射正确性**：
    - 地图模式：前方放置障碍物，对应区域触点是否正确升高
    - 边界条件：自身位置（第21排第11列）是否始终为Level 0
3. **异常处理**：
    - BLE断连时，触点是否复位至Level 0并边缘闪烁
    - 电量<20%时，是否触发低电量警告（APP推送+触点闪烁3次）
    - 相机权限未授予时，APP是否弹窗提示并引导用户授权

**验收标准**：所有测试项通过率>95%。

### 3.4.2 性能验证
**验证目标**：确保系统实时性、稳定性、续航时间满足设计指标。

**测试项**：

1. **端到端延迟**：
    - 测量方法：在障碍物前放置标记物，用高速相机记录障碍物移动与触点升高的时间差
    - 目标：<300ms
2. **刷新率**：
    - 测量方法：记录1分钟内grid数组更新次数
    - 目标：地图模式3-5Hz，放大模式1-2Hz
3. **丢包率**：
    - 测量方法：记录1000帧数据中，seq序号不连续的次数
    - 目标：<1%
4. **续航时间**：
    - 测量方法：满电状态下连续运行至电量耗尽（自动关机）
    - 目标：>4小时

**验收标准**：所有指标达标率>90%。

### 3.4.3 场景验证
**验证目标**：确保系统在真实出行场景下可用、可靠、用户体验良好。

**测试场景**：

1. **室内走廊**：识别墙面、门框、消防栓等静态障碍
2. **人行道**：识别行人、自行车、路沿、垃圾桶等动态+静态障碍
3. **复杂路口**：识别多方向车辆、行人、红绿灯杆等密集障碍
4. **商场入口**：识别自动门、台阶、导向牌等结构信息
5. **夜间场景**：验证弱光环境下深度估计精度
6. **雨天场景**：验证镜头防水与雨滴干扰处理
7. **临时障碍**：识别施工围挡、共享单车等动态障碍

**测试方法**：

+ 每个场景测试3次，记录触觉反馈准确性（正确识别障碍物个数/实际障碍物总数）
+ 邀请5位视障用户体验，收集主观评价（5分制）

**验收标准**：

+ 准确率>80%（即10个障碍物中能正确识别8个）
+ 用户体验评分>4分（满分5分）

## 3.5 总结与展望
灵触·随行项目通过创新的结构化触觉地图编码、双模式自适应输出、渐进式感知接入等技术方案，为视障人群提供了一种全新的环境感知与辅助导航解决方案。项目具备需求真实、技术成熟、工程可行、成本可控的特点，采用三阶段迭代开发策略，在保证核心功能可用性的前提下，灵活平衡性能、成本与开发周期。

**当前阶段成果**：

+ 完成需求调研与竞品分析，明确技术路线
+ 完成系统架构设计与核心数据协议定义
+ 完成MVP原型验证，证明技术路线可行

**后续工作重点**：

+ 完善双模式功能，提升触觉反馈准确性与实时性
+ 扩展触觉点阵规模至31×21全尺寸，优化硬件集成
+ 开展用户测试，根据反馈迭代优化用户体验
+ 完成演示准备，形成可复现的展示流程

**长远愿景**：

未来，我们计划将项目扩展至以下方向：

1. **多传感器融合**：接入ToF、毫米波雷达等传感器，提升全天候感知能力
2. **室内定位与导航**：结合室内定位技术（如蓝牙信标），实现建筑内精准导航
3. **云端协作与众包**：用户上传障碍物数据，构建“视障友好地图”，实现社区共建
4. **商业化探索**：与辅具企业合作，推动产品量产与市场推广，让技术真正惠及用户

我们相信，通过技术创新与社会关怀的结合，视障人群的出行体验一定能够得到根本性改善，独立、安全、有尊严的出行将不再是奢望，而是每个人都应享有的基本权利。



## 参考资料
[1] WeWALK. _WeWALK Smart Cane_ [EB/OL]. [https://wewalk.io/](https://wewalk.io/).

[2] Velázquez R. Wearable assistive devices for the blind [C]// _Wearable Technologies: Challenges and Opportunities_. Cham: Springer, 2016: 75–101.

[3] Park S U, Lee H K, Kim H B, et al. Wearable interactive full-body motion tracking and haptic feedback network systems with deep learning [J]. _Nature Communications_, 2025, 16: 8604.

[4] Yang L, Kang B, Huang Z, et al. Depth Anything: Unleashing the power of large-scale unlabeled data [C]// _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2024.

[5] Jocher G, Chaurasia A, Qiu J. _Ultralytics YOLOv8_ [EB/OL]. Version 8.0.0, 2023. [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics).

[6] Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? The KITTI vision benchmark suite [C]// _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. 2012.

[7] Geiger A, Lenz P, Stiller C, et al. Vision meets robotics: The KITTI dataset [J]. _The International Journal of Robotics Research_, 2013, 32(11): 1231–1237.

[8] Silberman N, Hoiem D, Kohli P, et al. Indoor segmentation and support inference from RGB-D images [C]// _Proceedings of the European Conference on Computer Vision (ECCV)_. 2012.

[9] Cordts M, Omran M, Ramos S, et al. The Cityscapes dataset for semantic urban scene understanding [C]// _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. 2016: 3213–3223.

[10] Low-voltage wearable tactile display with thermo-pneumatic actuation [J]. _npj Flexible Electronics_, 2025. DOI:10.1038/s41528-025-00426-3.

[11] Zhao Y, Lv W, Xu S, et al. DETRs beat YOLOs on real-time object detection [C]// _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2024.

[12]  Scharstein D, Szeliski R. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms [J]. International Journal of Computer Vision, 2002, 47(1): 7-42.  

[13] 国家统计局. 第二次全国残疾人抽样调查主要数据公报 [R]. 北京: 国家统计局, 2007.

[14] 中国盲人协会. 中国互联网视障用户基本情况报告 [R]. 北京: 中国盲人协会, 2020.

[15] Fischler M A, Bolles R C. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography [J]. _Communications of the ACM_, 1981, 24(6): 381–395.

[16] Dot Inc. _Dot Pad: The first smart tactile graphics display for the visually impaired_ [EB/OL]. 2023.

[17] Gomez C, Oller J, Paradells J. Overview and evaluation of Bluetooth Low Energy: An emerging low-power wireless technology [J]. _Sensors_, 2012, 12(9): 11734–11753.

[18] Ranftl R, Bochkovskiy A, Koltun V. Vision transformers for dense prediction [C]// _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_. 2021.

[19] Wang Y. _MobileDepth: Efficient monocular depth prediction on mobile devices_ [EB/OL]. 2020. arXiv:2011.10189.

[20] Hirschmuller H. Stereo processing by semiglobal matching and mutual information [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30(2): 328-341.

[21] Koenig S, Likhachev M. D* lite [C]// Proceedings of the 18th National Conference on Artificial Intelligence (AAAI). Edmonton, Alberta, Canada: AAAI Press, 2002: 476-483.

[22] 中国残联. 2021-2022年残疾人基本康复服务数据 [R]. 北京: 中国残疾人联合会, 2023.  

